{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding max lengths in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def calculate_max_lengths(data_path, tokenizer):\n",
    "    max_prompt_length = 0\n",
    "    max_target_length = 0\n",
    "    max_combined_length = 0\n",
    "\n",
    "    with open(data_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            prompt_tokens = tokenizer.encode(data['prompt'], add_special_tokens=False)\n",
    "            chosen_tokens = tokenizer.encode(data['chosen'], add_special_tokens=False)\n",
    "            rejected_tokens = tokenizer.encode(data['rejected'], add_special_tokens=False)\n",
    "\n",
    "            prompt_length = len(prompt_tokens)\n",
    "            chosen_length = len(chosen_tokens)\n",
    "            rejected_length = len(rejected_tokens)\n",
    "\n",
    "            max_prompt_length = max(max_prompt_length, prompt_length)\n",
    "            max_target_length = max(max_target_length, chosen_length, rejected_length)\n",
    "            max_combined_length = max(max_combined_length, prompt_length + chosen_length, prompt_length + rejected_length)\n",
    "\n",
    "    return max_prompt_length, max_target_length, max_combined_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data - Max Prompt Length: 528, Max Target Length: 1351, Max Combined Length: 1647\n",
      "Validation Data - Max Prompt Length: 474, Max Target Length: 991, Max Combined Length: 1302\n",
      "Overall - Max Prompt Length: 528, Max Target Length: 1351, Max Combined Length: 1647\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths to your datasets\n",
    "train_data_path = 'datasets/DPO_train.jsonl'\n",
    "vali_data_path = 'datasets/DPO_eval.jsonl'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Calculate max lengths for training data\n",
    "train_max_prompt_length, train_max_target_length, train_max_combined_length = calculate_max_lengths(train_data_path, tokenizer)\n",
    "print(f'Training Data - Max Prompt Length: {train_max_prompt_length}, Max Target Length: {train_max_target_length}, Max Combined Length: {train_max_combined_length}')\n",
    "\n",
    "# Calculate max lengths for validation data\n",
    "vali_max_prompt_length, vali_max_target_length, vali_max_combined_length = calculate_max_lengths(vali_data_path, tokenizer)\n",
    "print(f'Validation Data - Max Prompt Length: {vali_max_prompt_length}, Max Target Length: {vali_max_target_length}, Max Combined Length: {vali_max_combined_length}')\n",
    "\n",
    "# Determine overall max lengths\n",
    "overall_max_prompt_length = max(train_max_prompt_length, vali_max_prompt_length)\n",
    "overall_max_target_length = max(train_max_target_length, vali_max_target_length)\n",
    "overall_max_combined_length = max(train_max_combined_length, vali_max_combined_length)\n",
    "\n",
    "print(f'Overall - Max Prompt Length: {overall_max_prompt_length}, Max Target Length: {overall_max_target_length}, Max Combined Length: {overall_max_combined_length}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Training Data - Max Prompt Length: 528, Max Target Length: 1351, Max Combined Length: 1647\n",
    "\n",
    "Validation Data - Max Prompt Length: 474, Max Target Length: 991, Max Combined Length: 1302\n",
    "\n",
    "Overall - Max Prompt Length: 528, Max Target Length: 1351, Max Combined Length: 1647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-neo: DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duzan/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHI 3: DPO and LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel '.venv (Python 3.9.18)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unknown system error -122: Unknown system error -122, open '/users/eleves-b/2024/clement.dumas/.local/share/jupyter/runtime/kernel-v2-1468165UH2UflZv46uZ.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftType\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "train_data_path = 'datasets/DPO_train.jsonl'\n",
    "vali_data_path = 'datasets/DPO_eval.jsonl'\n",
    "\n",
    "# Load train and eval datasets\n",
    "train_path = 'datasets/DPO_train.jsonl'\n",
    "eval_path = 'datasets/DPO_eval.jsonl'\n",
    "dataset = load_dataset('json', data_files={\"train\": train_path, \"evaluation\": eval_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magatha-duzan\u001b[0m (\u001b[33magatha-duzan-EPFL\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/eleves-b/2024/clement.dumas/project-m3-2024-agatha-duzan/model/wandb/run-20240706_191911-1d61pjre</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/agatha-duzan-EPFL/dpo_lora/runs/1d61pjre' target=\"_blank\">fluent-armadillo-1</a></strong> to <a href='https://wandb.ai/agatha-duzan-EPFL/dpo_lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/agatha-duzan-EPFL/dpo_lora' target=\"_blank\">https://wandb.ai/agatha-duzan-EPFL/dpo_lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/agatha-duzan-EPFL/dpo_lora/runs/1d61pjre' target=\"_blank\">https://wandb.ai/agatha-duzan-EPFL/dpo_lora/runs/1d61pjre</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/agatha-duzan-EPFL/dpo_lora/runs/1d61pjre?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff38478b430>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play with LORA configuration: start from hyperparams seen in literature, try different values later\n",
    "lora_config = LoraConfig(\n",
    "    r= 32, # attention dimension; default is 8, try higher for more precision\n",
    "    lora_alpha=16, # decrease it if we see unstable\n",
    "    target_modules='all-linear', # try with all-linear for more radical changes \n",
    "    lora_dropout=0.01, #same as original LORA paper\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "wandb.init(project=\"dpo_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,\n",
    "    label_smoothing=0,\n",
    "    loss_type=\"sigmoid\",\n",
    "    precompute_ref_log_probs=True,\n",
    "    max_length=1647,\n",
    "    max_prompt_length=528,\n",
    "    max_target_length=1351, # calculated in dpo_debug\n",
    "    disable_dropout=True,\n",
    "    generate_during_eval=False,\n",
    "    truncation_mode=\"keep_end\",\n",
    "    output_dir='checkpoints/dpo_lora',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to='wandb',\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=lora_model,\n",
    "    ref_model=None,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['evaluation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=None,\n",
    "    optimizers=(None, None)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
