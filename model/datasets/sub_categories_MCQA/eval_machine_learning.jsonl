{"subject":"machine_learning","question":"Question: Statement 1| Since the VC dimension for an SVM with a Radial Base Kernel is infinite, such an SVM must be worse than an SVM with polynomial kernel which has a finite VC dimension. Statement 2| A two layer neural network with linear activation functions is essentially a weighted combination of linear separators, trained on a given dataset; the boosting algorithm built on linear separators also finds a combination of linear separators, therefore these two algorithms will give the same result.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"B"}
{"subject":"machine_learning","question":"Question: Statement 1| The maximum margin decision boundaries that support vector machines construct have the lowest generalization error among all linear classifiers. Statement 2| Any decision boundary that we get from a generative model with classconditional Gaussian distributions could in principle be reproduced with an SVM and a polynomial kernel of degree less than or equal to three.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"D"}
{"subject":"machine_learning","question":"Question: Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"B"}
{"subject":"machine_learning","question":"Question: Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"D"}
{"subject":"machine_learning","question":"Question: Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatments. What kind of learning problem is this?\n\nOptions:\nA. Supervised learning\nB. Unsupervised learning\nC. Both (a) and (b)\nD. Neither (a) nor (b)\n\nAnswer:","answer":"B"}
{"subject":"machine_learning","question":"Question: The disadvantage of Grid search is?\n\nOptions:\nA. It can not be applied to non-differentiable functions.\nB. It can not be applied to non-continuous functions.\nC. It is hard to implement.\nD. It runs reasonably slow for multiple linear regression.\n\nAnswer:","answer":"D"}
{"subject":"machine_learning","question":"Question: Statement 1| In AdaBoost weights of the misclassified examples go up by the same multiplicative factor. Statement 2| In AdaBoost, weighted training error e_t of the tth weak classifier on training data with weights D_t tends to increase as a function of t.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Given two Boolean random variables, A and B, where P(A) = 1\/2, P(B) = 1\/3, and P(A | \u00acB) = 1\/4, what is P(A | B)?\n\nOptions:\nA. 1\/6\nB. 1\/4\nC. 3\/4\nD. 1\n\nAnswer:","answer":"D"}
{"subject":"machine_learning","question":"Question: As the number of training examples goes to infinity, your model trained on that data will have:?\n\nOptions:\nA. Lower variance\nB. Higher variance\nC. Same variance\nD. None of the above\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Statement 1| The softmax function is commonly used in mutliclass logistic regression. Statement 2| The temperature of a nonuniform softmax distribution affects its entropy.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Which of the following is NOT supervised learning?\n\nOptions:\nA. PCA\nB. Decision Tree\nC. Linear Regression\nD. Naive Bayesian\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Statement 1| VGGNets have convolutional kernels of smaller width and height than AlexNet's first-layer kernels. Statement 2| Data-dependent weight initialization procedures were introduced before Batch Normalization.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Which of the following is true of a convolution kernel?\n\nOptions:\nA. Convolving an image with $\\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ would not change the image\nB. Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image\nC. Convolving an image with $\\begin{bmatrix}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ would not change the image\nD. Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image\n\nAnswer:","answer":"B"}
{"subject":"machine_learning","question":"Question: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\n\nOptions:\nA. 0\nB. 1\nC. 2\nD. 3\n\nAnswer:","answer":"C"}
{"subject":"machine_learning","question":"Question: For Kernel Regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:\n\nOptions:\nA. Whether kernel function is Gaussian versus triangular versus box-shaped\nB. Whether we use Euclidian versus L1 versus L\u221e metrics\nC. The kernel width\nD. The maximum height of the kernel function\n\nAnswer:","answer":"C"}
{"subject":"machine_learning","question":"Question: Adding more basis functions in a linear model, pick the most probably option:\n\nOptions:\nA. Decreases model bias\nB. Decreases estimation bias\nC. Decreases variance\nD. Doesn\u2019t affect bias and variance\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatments. What kind of learning problem is this?\n\nOptions:\nA. Supervised learning\nB. Unsupervised learning\nC. Both (a) and (b)\nD. Neither (a) nor (b)\n\nAnswer:","answer":"B"}
{"subject":"machine_learning","question":"Question: We are training fully connected network with two hidden layers to predict housing prices. Inputs are $100$-dimensional, and have several features such as the number of square feet, the median family income, etc. The first hidden layer has $1000$ activations. The second hidden layer has $10$ activations. The output is a scalar representing the house price. Assuming a vanilla network with affine transformations and with no batch normalization and no learnable parameters in the activation function, how many parameters does this network have?\n\nOptions:\nA. 111021\nB. 110010\nC. 111110\nD. 110011\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Statement 1| In AdaBoost weights of the misclassified examples go up by the same multiplicative factor. Statement 2| In AdaBoost, weighted training error e_t of the tth weak classifier on training data with weights D_t tends to increase as a function of t.\n\nOptions:\nA. True, True\nB. False, False\nC. True, False\nD. False, True\n\nAnswer:","answer":"A"}
{"subject":"machine_learning","question":"Question: Which of the following is more appropriate to do feature selection?\n\nOptions:\nA. Ridge\nB. Lasso\nC. both (a) and (b)\nD. neither (a) nor (b)\n\nAnswer:","answer":"B"}
{"subject":"machine_learning","question":"Question: To achieve an 0\/1 loss estimate that is less than 1 percent of the true 0\/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?\n\nOptions:\nA. around 10 examples\nB. around 100 examples\nC. between 100 and 500 examples\nD. more than 1000 examples\n\nAnswer:","answer":"D"}
