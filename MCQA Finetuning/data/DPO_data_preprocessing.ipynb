{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crafting the DPO dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data format\n",
    "We need preference pairs for DPO, so we want the final data format to look like this:\n",
    "\n",
    "`{\"prompt\": \"our_question\", \"chosen\": \"best_answer\", \"rejected\": \"less_good_answer\"}`\n",
    "\n",
    "We will use the preference data from milestone 1, and process it to create our preference dataset.\n",
    "\n",
    "We will also add preference data from [EXTERNAL SOURCES] : TBD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. M1 preference data preprocessing\n",
    "Loading data from milestone 1 and processing it to the desired format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "Let's have a look at the data we have from milestone 1, and see how we can use it to create our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_complete</th>\n",
       "      <th>course_id</th>\n",
       "      <th>preference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Question: Consider the following contains func...</td>\n",
       "      <td>15000</td>\n",
       "      <td>[{'A': 'The asymptotic depth of the contains f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Question: What is the asymptotic work of &lt;code...</td>\n",
       "      <td>15000</td>\n",
       "      <td>[{'A': '...', 'B': '...', 'overall': 'A', 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Question: We have a collection of rectangles i...</td>\n",
       "      <td>15000</td>\n",
       "      <td>[{'A': 'Facts:\n",
       "- Rectangles in the plane have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Question: Which of the following scheduler pol...</td>\n",
       "      <td>15005</td>\n",
       "      <td>[{'A': 'Preemptive scheduling policies allow t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Question: In this week's lecture, you have bee...</td>\n",
       "      <td>15000</td>\n",
       "      <td>[{'A': 'For the computation g(g(1, x1), g(x2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                  question_complete  course_id  \\\n",
       "0            0  Question: Consider the following contains func...      15000   \n",
       "1            3  Question: What is the asymptotic work of <code...      15000   \n",
       "2            4  Question: We have a collection of rectangles i...      15000   \n",
       "3            5  Question: Which of the following scheduler pol...      15005   \n",
       "4            7  Question: In this week's lecture, you have bee...      15000   \n",
       "\n",
       "                                          preference  \n",
       "0  [{'A': 'The asymptotic depth of the contains f...  \n",
       "1  [{'A': '...', 'B': '...', 'overall': 'A', 'cri...  \n",
       "2  [{'A': 'Facts:\n",
       "- Rectangles in the plane have ...  \n",
       "3  [{'A': 'Preemptive scheduling policies allow t...  \n",
       "4  [{'A': 'For the computation g(g(1, x1), g(x2, ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"raw data/M1_preference_data_15052024.json\"\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data\n",
    "See how it is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'question_complete', 'course_id', 'preference'], dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_id                                                       1205\n",
       "question_complete    Question: Consider the following matrix-factor...\n",
       "course_id                                                        15127\n",
       "preference           [{'A': 'To find the optimal value of $b_u$, we...\n",
       "Name: 155, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.iloc[155]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A': 'To find the optimal value of $b_u$, we need to minimize the objective function with respect to $b_u$. \\n\\nTaking the derivative of the objective function with respect to $b_u$ and setting it to zero, we get:\\n$$\\n\\\\sum_{u \\\\sim m} (f_{um} - r_{um})\\\\frac{\\\\partial f_{um}}{\\\\partial b_u} + \\\\lambda b_u = 0\\n$$\\n\\nSince $f_{um} = \\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle + b_u + b_m$, the partial derivative with respect to $b_u$ is simply 1. Therefore, the equation simplifies to:\\n$$\\n\\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_u = 0\\n$$\\n\\nRearranging the terms, we get:\\n$$\\nb_u = \\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (r_{um} - \\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle - b_m)\\n$$\\n\\nTherefore, the optimal value of $b_u$ is given by the above expression.',\n",
       "  'B': \"To find the optimal value of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$. Let's take the derivative of the objective function with respect to $b_{u}$ and set it to zero to find the optimal value.\\n\\nTaking the derivative of the objective function with respect to $b_{u}$, we get:\\n\\n$$ \\\\sum_{u \\\\sim m}(f_{u m}-r_{u m}) + \\\\lambda b_{u} = 0 $$\\n\\nSince all other values are fixed, we can simplify this further:\\n\\n$$ \\\\sum_{u \\\\sim m}(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle + b_{u} + b_{m} - r_{u m}) + \\\\lambda b_{u} = 0 $$\\n\\nNow, we need to isolate $b_{u}$ to find its optimal value:\\n\\n$$ \\\\sum_{u \\\\sim m}(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle - r_{u m}) + \\\\lambda b_{u} = -\\\\sum_{u \\\\sim m}(b_{m}) $$\\n\\n$$ \\\\lambda b_{u} = -\\\\sum_{u \\\\sim m}(b_{m}) - \\\\sum_{u \\\\sim m}(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle - r_{u m}) $$\\n\\n$$ b_{u} = -\\\\frac{\\\\sum_{u \\\\sim m}(b_{m})}{\\\\lambda} - \\\\frac{\\\\sum_{u \\\\sim m}(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle - r_{u m})}{\\\\lambda} $$\\n\\nTherefore, the optimal value of $b_{u}$ is given by the above equation.\",\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'A',\n",
       "   'relevance': 'A',\n",
       "   'clarity': 'A',\n",
       "   'completeness': 'A',\n",
       "   'other': ''}},\n",
       " {'A': 'To find the optimal value of $b_{u}$, we need to take the derivative of the objective function with respect to $b_{u}$ and set it equal to 0.\\\\n\\\\nThe objective function can be written as:\\\\n\\\\n$$E = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^2 + \\\\|\\\\mathbf{v}_{u}\\\\|^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_{m}^2 + \\\\|\\\\mathbf{w}_{m}\\\\|^2)\\\\right]$$\\\\n\\\\nExpanding the first term and substituting the expression for $f_{um}$:\\\\n\\\\n$$E = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} \\\\left(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle + b_{u} + b_{m} - r_{um}\\\\right)^2 + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^2 + \\\\|\\\\mathbf{v}_{u}\\\\|^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_{m}^2 + \\\\|\\\\mathbf{w}_{m}\\\\|^2)\\\\right]$$\\\\n\\\\nTaking the derivative with respect to $b_{u}$ and setting it equal to 0:\\\\n\\\\n$$\\\\frac{\\\\partial E}{\\\\partial b_{u}} = \\\\sum_{u \\\\sim m} \\\\left(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle + b_{u} + b_{m} - r_{um}\\\\right) + \\\\lambda b_{u} = 0$$\\\\n\\\\nSolving for $b_{u}$, we get:\\\\n\\\\n$$b_{u} = -\\\\frac{1}{\\\\lambda} \\\\left[\\\\sum_{u \\\\sim m} \\\\left(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle + b_{m} - r_{um}\\\\right)\\\\right]$$\\\\n\\\\nTherefore, the optimal value of $b_{u}$ is given by the above expression.',\n",
       "  'B': \"To find the optimal values of $b_u$ in the given objective function, we need to minimize the objective function with respect to $b_u$. Let's derive the optimal value step by step:\\\\n\\\\nGiven:\\\\n- $f_{u m} = \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle + b_{u} + b_{m}$\\\\n- Objective function:\\\\n$$\\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m})^2 + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^2 + \\\\|\\\\mathbf{v}_{u}\\\\|^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_{m}^2 + \\\\|\\\\mathbf{w}_{m}\\\\|^2)\\\\right]$$\\\\n\\\\n1. Compute the partial derivative of the objective function with respect to $b_u$:\\\\n$$\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left[\\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m})^2\\\\right] = \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m})$$\\\\n$$\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left[\\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^2 + \\\\|\\\\mathbf{v}_{u}\\\\|^2)\\\\right] = \\\\lambda b_{u}$$\\\\n\\\\n2. Set the total derivative equal to 0 and solve for $b_u$:\\\\n$$\\\\sum_{u \\\\sim m} (f_{u m} - r_{u m}) + \\\\lambda b_{u} = 0$$\\\\n$$\\\\sum_{u \\\\sim m} (b_{u} + \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle - r_{u m}) + \\\\lambda b_{u} = 0$$\\\\n$$\\\\sum_{u \\\\sim m} b_{u} = \\\\sum_{u \\\\sim m} (r_{u m} - \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle)$$\\\\n\\\\n3. Finally, the optimal value of $b_u$ is:\\\\n$$b_{u} = \\\\frac{1}{\\\\lambda} \\\\left(\\\\sum_{u \\\\sim m} (r_{u m} - \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle)\\\\right)$$\\\\n\\\\nIn summary, the optimal value of $b_u$ is obtained by summing the residuals $r_{u m} - \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle$ for all pairs $(u, m)$ and dividing by $\\\\lambda$. This process minimizes the objective function with respect to $b_u$.\",\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'None',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'A',\n",
       "   'completeness': 'AB',\n",
       "   'other': ''}},\n",
       " {'A': \"To find the optimal value of $b_u$, let's first rewrite the objective function in terms of $b_u$ separately. We will differentiate it with respect to $b_u$ and set the derivative to zero to find the optimal value of $b_u$.\\n\\nGiven:\\n$$\\nf_{u m} = \\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle + b_u + b_m\\n$$\\nObjective function:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m})^2 + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2 + \\\\|\\\\mathbf{v}_u\\\\|^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_m^2 + \\\\|\\\\mathbf{w}_m\\\\|^2)\\\\right]\\n$$\\n\\nLet's focus on the terms dependent on $b_u$:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m})^2 + \\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2 + \\\\|\\\\mathbf{v}_u\\\\|^2)\\n$$\\n\\nExpanding the first term and isolating the terms with $b_u$:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m} ((\\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle + b_u + b_m) - r_{u m})^2 = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} ((\\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle + b_m) - r_{u m})^2 + \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (b_u)^2 + \\\\text{other terms not involving } b_u\\n$$\\n\\nExpanding the second term and isolating the term with $b_u$:\\n$$\\n\\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2 + \\\\|\\\\mathbf{v}_u\\\\|^2) = \\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2) + \\\\text{other terms not involving } b_u\\n$$\\n\\nCombining the terms with $b_u$ from both parts:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m} (b_u)^2 + \\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2) = \\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m} 1 + \\\\frac{\\\\lambda}{2} |\\\\mathbf{U}|\\\\right) (b_u)^2\\n$$\\n\\nNow, to find the optimal value of $b_u$, we differentiate the combined term with respect to $b_u$ and set it to zero:\\n$$\\n\\\\frac{d}{db_u} \\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m} 1 + \\\\frac{\\\\lambda}{2} |\\\\mathbf{U}| \\\\right) (b_u)^2 = 0\\n$$\\n\\nSolving for $b_u$:\\n$$\\n\\\\left(\\\\sum_{u \\\\sim m} 1 + \\\\lambda |\\\\mathbf{U}| \\\\right) b_u = 0\\n$$\\n\\nThus, the optimal value of $b_u$ is:\\n$$\\nb_u = \\\\frac{\\\\sum_{u \\\\sim m} r_{u m}}{\\\\sum_{u \\\\sim m} 1 + \\\\lambda |\\\\mathbf{U}|}\\n$$\\n\\nThis formula gives the optimal value of $b_u$ when all other values are fixed in the matrix-factorization problem described.\",\n",
       "  'B': 'To find the optimal value of $b_u$ while keeping all other values fixed, we take the derivative of the objective function with respect to $b_u$ and set it equal to zero. \\n\\nDifferentiating with respect to $b_u$, we get:\\n\\n$$\\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_u = 0$$\\n\\nRearranging the terms, we get:\\n\\n$$b_u = \\\\frac{\\\\sum_{u \\\\sim m} (r_{um} - \\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle - b_m - \\\\lambda b_u)}{\\\\lambda}$$\\n\\nTherefore, the optimal value of $b_u$ is given by the above expression.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'A',\n",
       "   'relevance': 'A',\n",
       "   'clarity': 'A',\n",
       "   'completeness': 'A',\n",
       "   'other': 'Conciseness: B; Engagement: A'}},\n",
       " {'A': 'To find the optimal value of $b_u$, we differentiate the objective function with respect to $b_u$ and set it to zero. \\n\\nTaking the derivative of the objective function with respect to $b_u$, we get:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left[\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right] \\\\right]=0$$\\n\\nExpanding the terms and simplifying, we get:\\n\\n$$\\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right) + \\\\lambda b_u = 0$$\\n\\nRearranging the terms, we get the optimal value for $b_u$ as:\\n\\n$$b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)$$\\n\\nTherefore, the optimal value of $b_u$ is given by the negative sum of the residuals divided by $\\\\lambda$.',\n",
       "  'B': \"To find the optimal value of $b_{u}$, we can minimize the objective function with respect to $b_{u}$ while keeping all other values fixed. Let's write out the objective function in terms of $b_{u}$ and the other terms:\\n\\n$$\\n\\\\text{Objective} = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{u m} \\\\right)^2 + \\\\frac{\\\\lambda}{2} \\\\left[ \\\\sum_{u \\\\in \\\\mathbf{U}} \\\\left( b_{u}^{2} + \\\\left\\\\| \\\\mathbf{v}_{u} \\\\right\\\\|^2 \\\\right) + \\\\sum_{m \\\\in \\\\mathbf{M}} \\\\left( b_{m}^{2} + \\\\left\\\\| \\\\mathbf{w}_{m} \\\\right\\\\|^2 \\\\right) \\\\right]\\n$$\\n\\nNow, let's focus on the terms involving $b_{u}$:\\n\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{u m} \\\\right)^2 + \\\\frac{\\\\lambda}{2} \\\\left[ b_{u}^{2} + \\\\left\\\\| \\\\mathbf{v}_{u} \\\\right\\\\|^2 \\\\right]\\n$$\\n\\nTo find the optimal value of $b_{u}$, we take the derivative of the objective function with respect to $b_{u}$ and set it to zero:\\n\\n$$\\n\\\\frac{\\\\partial \\\\text{Objective}}{\\\\partial b_{u}} = \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{u m} \\\\right) + \\\\lambda b_{u} = 0\\n$$\\n\\nSolving for $b_{u}$, we get:\\n\\n$$\\nb_{u} = -\\\\frac{1}{\\\\lambda} \\\\left( \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle - r_{u m} - b_{m} \\\\right) \\\\right)\\n$$\\n\\nTherefore, the optimal value of $b_{u}$ is given by this equation.\",\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'B',\n",
       "   'relevance': 'B',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'B',\n",
       "   'other': 'Conciseness: A; Engagement: B'}},\n",
       " {'A': 'To find the optimal value of $b_u$, we need to take the derivative of the objective function with respect to $b_u$ and set it equal to zero. The objective function is given by:\\n\\n$$\\\\frac{1}{2} \\\\sum_{u \\\\sim m}(f_{u m}-r_{u m})^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}(b_{u}^{2}+\\\\|\\\\mathbf{v}_{u}\\\\|^{2})+\\\\sum_{m \\\\in \\\\mathbf{M}}(b_{m}^{2}+\\\\|\\\\mathbf{w}_{m}\\\\|^{2})\\\\right]$$\\n\\nTaking the derivative with respect to $b_u$, we get:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_u}\\\\left[\\\\frac{1}{2} \\\\sum_{u \\\\sim m}(f_{u m}-r_{u m})^{2}+\\\\frac{\\\\lambda}{2}\\\\sum_{u \\\\in \\\\mathbf{U}}(b_{u}^{2}+\\\\|\\\\mathbf{v}_{u}\\\\|^{2})\\\\right] = \\\\lambda b_u + \\\\sum_{m \\\\sim u}(f_{u m}-r_{u m})$$\\n\\nSetting this derivative equal to zero and solving for $b_u$, we get:\\n\\n$$\\\\lambda b_u + \\\\sum_{m \\\\sim u}(f_{u m}-r_{u m}) = 0$$\\n$$b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{m \\\\sim u}(f_{u m}-r_{u m})$$\\n\\nTherefore, the optimal value of $b_u$ is given by the above expression.',\n",
       "  'B': \"To find the optimal value of $b_{u}$, we differentiate the objective function with respect to $b_{u}$ and set it to zero. Let's calculate the derivative:\\n\\nGiven the objective function: \\n\\n$$ \\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right] $$\\n\\nThe partial derivative of the objective function with respect to $b_{u}$ is:\\n\\n$$ \\\\frac{\\\\partial}{\\\\partial b_{u}} = \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_{u} $$\\n\\nSetting this derivative to zero, we get:\\n\\n$$ \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_{u} = 0 $$\\n\\nSolving for $b_{u}$, we get:\\n\\n$$ b_{u} = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) $$\\n\\nTherefore, the optimal value of $b_{u}$, provided that all other values are fixed, is:\\n\\n$$ b_{u} = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) $$\\n\\nThis is the expression for updating the bias term $b_{u}$ in the matrix factorization problem.\",\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'A',\n",
       "   'completeness': 'A',\n",
       "   'other': 'Conciseness: B; Engagement: A'}},\n",
       " {'A': \"To find the optimal value of $b_u$ while keeping all other values fixed, we need to take the derivative of the objective function with respect to $b_u$ and set it to zero.\\n\\nTaking the derivative of the objective function with respect to $b_u$, we get:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left( \\\\frac{1}{2} \\\\sum_{u \\\\sim m}(f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in U} (b_u^2 + ||v_u||^2) + \\\\sum_{m \\\\in M} (b_m^2 + ||w_m||^2) \\\\right)$$\\n\\nNow let's focus on the terms that involve $b_u$:\\n\\n1. The first term in the objective function involves $f_{um}$, which in turn involves $b_u$. The derivative of this term with respect to $b_u$ is $2(f_{um} - r_{um})$.\\n\\n2. The second term in the objective function involves $b_u$ squared. The derivative of this term with respect to $b_u$ is $\\\\lambda b_u$.\\n\\nSetting the sum of these derivatives to zero, we get:\\n\\n$$\\\\sum_{u \\\\sim m}(f_{um} - r_{um}) + \\\\lambda b_u = 0$$\\n\\nSolving for $b_u$:\\n\\n$$b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m}(f_{um} - r_{um})$$\\n\\nTherefore, the optimal value for $b_u$ is given by the above expression.\",\n",
       "  'B': 'To find the optimal value of $b_{u}$, we can take the derivative of the objective function with respect to $b_{u}$ and set it to zero:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_{u}} \\\\left[\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\\\right] = 0$$\\n\\nSolving this equation will give us the optimal value of $b_{u}$.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'None',\n",
       "   'completeness': 'None',\n",
       "   'other': ''}},\n",
       " {'A': 'To find the optimal value of $b_u$, we take the derivative of the objective function with respect to $b_u$ and set it equal to zero. \\n\\nTaking the derivative of the objective function with respect to $b_u$, we get:\\n\\n$$ \\\\frac{\\\\partial}{\\\\partial b_u} \\\\left( \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in U} (b_u^2 + ||v_u||^2) + \\\\sum_{m \\\\in M} (b_m^2 + ||w_m||^2) \\\\right) $$\\n\\nSince $f_{um} = \\\\langle v_u, w_m \\\\rangle + b_u + b_m$, the derivative with respect to $b_u$ simplifies to:\\n\\n$$ \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_u $$\\n\\nSetting this equal to zero and solving for $b_u$, we get:\\n\\n$$ \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_u = 0 $$\\n\\n$$ b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) $$\\n\\nTherefore, the optimal value of $b_u$ is given by $b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})$.',\n",
       "  'B': \"To find the optimal value of $b_{u}$, we need to take the derivative of the objective function with respect to $b_{u}$ and set it to zero. Let's start by calculating the derivative of the objective function with respect to $b_{u}$:\\n\\n$$ \\\\frac{\\\\partial}{\\\\partial b_{u}} \\\\left( \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\left[ \\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^2 + \\\\|\\\\mathbf{v}_{u}\\\\|^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_{m}^2 + \\\\|\\\\mathbf{w}_{m}\\\\|^2) \\\\right] \\\\right) $$\\n\\nThe derivative of the first term inside the summation is straightforward:\\n\\n$$ \\\\frac{\\\\partial}{\\\\partial b_{u}} \\\\left( \\\\frac{1}{2} (f_{um} - r_{um})^2 \\\\right) = (f_{um} - r_{um}) $$\\n\\nNow, let's find the derivative of the second term inside the summation with respect to $b_{u}$:\\n\\n$$ \\\\frac{\\\\partial}{\\\\partial b_{u}} \\\\left( \\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^2 + \\\\|\\\\mathbf{v}_{u}\\\\|^2) \\\\right) = \\\\lambda b_{u} $$\\n\\nSetting the derivative equal to zero gives us the optimal value for $b_{u}$:\\n\\n$$ \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_{u} = 0 $$\\n\\nSolving for $b_{u}$, we get:\\n\\n$$ b_{u} = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) $$\\n\\nSo, the optimal value of $b_{u}$ is given by the negative sum over all pairs $(u, m)$ for which a rating exists of the difference between the estimated score $f_{um}$ and the observed rating $r_{um}$, divided by $\\\\lambda$.\",\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'B',\n",
       "   'other': 'conciseness: A'}},\n",
       " {'A': \"To find the optimal value of $b_u$ given that all other values are fixed, we can treat this problem as an optimization problem. We want to minimize the objective function with respect to $b_u$. \\n\\nFirst, let's rewrite the objective function in terms of $b_u$:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle + b_{u} + b_{m} - r_{u m}\\\\right)^{2} + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^{2} + \\\\|\\\\mathbf{v}_{u}\\\\|^{2}) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_{m}^{2} + \\\\|\\\\mathbf{w}_{m}\\\\|^{2})\\\\right]\\n$$\\n\\nNow, we want to find the optimal value of $b_u$ that minimizes this objective function. To do this, we need to take the derivative of the objective function with respect to $b_u$ and set it equal to 0. \\n\\nTaking the derivative with respect to $b_u$, we get:\\n$$\\n\\\\sum_{u \\\\sim m}(\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle + b_{u} + b_{m} - r_{u m}) + \\\\lambda b_{u} = 0\\n$$\\n\\nSolving for $b_u$, we get:\\n$$\\nb_{u} = \\\\frac{\\\\sum_{u \\\\sim m}(r_{u m} - \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle - b_{m})}{\\\\lambda}\\n$$\\n\\nTherefore, the optimal value of $b_u$ that minimizes the objective function is given by the above equation. This value can be calculated iteratively by updating $b_u$ until convergence is reached.\",\n",
       "  'B': 'The main mathematical topics in this problem involve matrix factorization, optimization, and regularization. In matrix factorization, we aim to estimate the unknown ratings by decomposing the matrix of observed ratings into two lower-dimensional matrices representing users and items. The optimization objective involves minimizing the squared error between the estimated and observed ratings, along with a regularization term to prevent overfitting.\\n\\nKey Points:\\n1. Matrix factorization is a common technique used in recommendation systems to estimate unknown ratings.\\n2. The optimization objective includes a squared error term and a regularization term to balance accuracy and generalization.\\n3. Regularization helps prevent overfitting by penalizing large values of biases and latent factors.\\n\\nBasic Cause-and-Effect:\\nTo find the optimal value of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$ while keeping all other variables fixed. Taking the derivative of the objective function with respect to $b_{u}$ and setting it to zero will give us the optimal value.\\n\\nMathematically, the derivative of the objective function with respect to $b_{u}$ is:\\n$$ \\\\frac{\\\\partial}{\\\\partial b_{u}}\\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\\\right) $$\\n\\nSetting this derivative to zero and solving for $b_{u}$ will give us the optimal value of $b_{u}$. This process involves finding the balance between minimizing the squared error term and the regularization term to achieve the best estimate of $b_{u}$ that generalizes well to unseen data.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'AB',\n",
       "   'other': ''}},\n",
       " {'A': 'To find the optimal value of $b_u$ while keeping all other values fixed, we can take the derivative of the objective function with respect to $b_u$ and set it equal to zero. \\n\\nTaking the derivative of the objective function with respect to $b_u$, we get:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left[\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\\\right]$$\\n\\nExpanding the first term in the objective function, we get:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left[\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right)^{2}\\\\right]$$\\n\\nSince $b_u$ only appears in the first term, we can focus on that term. Taking the derivative with respect to $b_u$, we get:\\n\\n$$\\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right) = 0$$\\n\\nSolving for $b_u$, we get:\\n\\n$$\\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right) = 0$$\\n$$\\\\sum_{u \\\\sim m} b_{u} = \\\\sum_{u \\\\sim m} (r_{u m} - \\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - b_{m})$$\\n$$b_{u} = \\\\frac{1}{\\\\text{number of ratings for user u}} \\\\sum_{u \\\\sim m} (r_{u m} - \\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - b_{m})$$\\n\\nTherefore, the optimal value of $b_u$ is the average difference between the observed rating and the predicted rating (based on the current values of $\\\\mathbf{v}_u$, $\\\\mathbf{w}_m$, and $b_m$) for all movies that user $u$ has rated.',\n",
       "  'B': 'To find the optimal value of $b_u$, we need to minimize the objective function with respect to $b_u$.\\n\\nThe objective function is:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\n$$\\n\\nTaking the derivative of the objective function with respect to $b_u$ and setting it to zero to find the optimal value:\\n$$\\n\\\\frac{\\\\partial}{\\\\partial b_u} = \\\\sum_{u \\\\sim m}(f_{um} - r_{um}) + \\\\lambda b_u = 0\\n$$\\n\\nSolving for $b_u$:\\n$$\\nb_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m}(f_{um} - r_{um})\\n$$\\n\\nTherefore, the optimal value of $b_u$ is given by $b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m}(f_{um} - r_{um})$.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'A',\n",
       "   'relevance': 'A',\n",
       "   'clarity': 'B',\n",
       "   'completeness': 'A',\n",
       "   'other': 'Conciseness: B; Engagement: AB'}},\n",
       " {'A': \"To find the optimal value of $b_{u}$, we need to take the derivative of the objective function with respect to $b_{u}$ and set it to zero. Let's denote the objective function as $L$.\\n\\nLet's take the derivative of $L$ with respect to $b_{u}$:\\n\\n$$\\\\frac{\\\\partial L}{\\\\partial b_{u}} = \\\\sum_{m}(f_{um} - r_{um}) + \\\\lambda b_{u}$$\\n\\nSetting this derivative to zero:\\n\\n$$\\\\sum_{m}(f_{um} - r_{um}) + \\\\lambda b_{u} = 0$$\\n\\n$$\\\\sum_{m}(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m} - r_{um}) + \\\\lambda b_{u} = 0$$\\n\\nSince we are looking for the optimal value of $b_{u}$, we can ignore the terms that do not involve $b_{u}$:\\n\\n$$\\\\sum_{m}(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - r_{um}) + \\\\sum_{m}b_{m} + \\\\lambda b_{u} = 0$$\\n\\nSince $\\\\sum_{m}b_{m}$ does not depend on $b_{u}$, we can treat it as a constant. Let's denote it as $C$:\\n\\n$$\\\\sum_{m}(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - r_{um}) + C + \\\\lambda b_{u} = 0$$\\n\\nNow, isolate $b_{u}$:\\n\\n$$\\\\lambda b_{u} = -\\\\left(\\\\sum_{m}(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - r_{um}) + C\\\\right)$$\\n\\n$$b_{u} = -\\\\frac{1}{\\\\lambda}\\\\left(\\\\sum_{m}(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - r_{um}) + C\\\\right)$$\\n\\nTherefore, the optimal value of $b_{u}$ is given by $b_{u} = -\\\\frac{1}{\\\\lambda}\\\\left(\\\\sum_{m}(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - r_{um}) + C\\\\right)$.\",\n",
       "  'B': 'To find the optimal value of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$. The objective function is given as:\\n\\n$$ \\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2} + \\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right] $$\\n\\nExpanding the terms involving $b_{u}$ and dropping the terms that do not involve $b_{u}$, we have:\\n\\n$$ \\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right)^{2} + \\\\frac{\\\\lambda}{2} \\\\left[b_{u}^{2} + \\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right] $$\\n\\nTaking the derivative of the objective function with respect to $b_{u}$ and setting it to zero, we find the optimal value for $b_{u}$:\\n\\n$$ \\\\sum_{u \\\\sim m} \\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle + b_{u} + b_{m} - r_{u m}\\\\right) + \\\\lambda b_{u} = 0 $$\\n\\nSolving for $b_{u}$, we get:\\n\\n$$ b_{u} = -\\\\frac{1}{\\\\lambda} \\\\left(\\\\sum_{u \\\\sim m} \\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - r_{u m}\\\\right) + C\\\\right) $$\\n\\nwhere $C$ is a constant term that does not depend on $b_{u}$.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'AB',\n",
       "   'other': ''}},\n",
       " {'A': 'The optimal value of $b_u$ is given by:\\n$$\\nb_u = \\\\frac{\\\\sum_{u \\\\sim m} (r_{u m} - \\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle - b_m) + \\\\lambda \\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}}{\\\\lambda}\\n$$',\n",
       "  'B': 'To find the optimal value of $b_u$ while keeping all other values fixed, we need to minimize the objective function with respect to $b_u$. \\n\\nGiven the objective function:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\n$$\\n\\nWe can see that the term involving $b_u$ is:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\n$$\\n\\nTo find the optimal value of $b_u$, we differentiate the above expression with respect to $b_u$ and set it to zero:\\n$$\\n\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left[\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\\\right] = 0\\n$$\\n\\nSolving this equation will give us the optimal value of $b_u$.',\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'B',\n",
       "   'completeness': 'AB',\n",
       "   'other': ''}},\n",
       " {'A': '##Answer:\\nThe optimal value of $b_{u}$, provided that all other values are fixed, is given by $$ b_{u} = \\\\frac{\\\\sum_{m \\\\sim u} (r_{u m} - \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle - b_{m})}{1 + \\\\lambda} $$\\n\\n##Reasons:\\n1. **Objective Function**: The given objective function consists of a least squares term for the difference between the estimated score $f_{u m}$ and the actual rating $r_{u m}$, as well as regularization terms for bias and vectors.\\n\\n2. **Optimization Task**: To find the optimal value of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$ while keeping all other variables fixed.\\n\\n3. **Partial Derivative**: Taking the partial derivative of the objective function with respect to $b_{u}$ and setting it to zero leads to the optimal value of $b_{u}$.\\n\\n4. **Derivation**: The derivative of the objective function with respect to $b_{u}$ results in the term $\\\\sum_{m \\\\sim u} (r_{u m} - \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\rangle - b_{m})$, which is then divided by $1 + \\\\lambda$ to obtain the optimal value of $b_{u}$.\\n\\n5. **Interpretation**: The optimal $b_{u}$ is calculated based on the errors between the estimated score for user $u$ on movie $m$ and the actual rating, adjusted by the regularization parameter $\\\\lambda$.\\n\\n6. **Regularization Factor**: The regularization term $\\\\lambda$ plays a role in controlling the impact of the regularization on the bias values, ensuring a balance between fitting the data and preventing overfitting.\\n\\n7. **Optimization Process**: By updating $b_{u}$ with the derived optimal value in an iterative optimization process, the objective function can be minimized to find the best bias values for the matrix-factorization problem.\\n\\nIn conclusion, the optimal value of $b_{u}$ in the matrix-factorization problem is determined by the errors in estimating the ratings, weighted by the regularization factor $\\\\lambda$, and adjusted based on the fixed values of other parameters in the model.',\n",
       "  'B': '##Answer:\\nThe optimal value of $b_u$ can be obtained by setting the derivative of the objective function with respect to $b_u$ to zero and solving for $b_u$. The optimal value is given by: $$ b_u = \\\\frac{1}{\\\\lambda + |I_u|} \\\\sum_{m \\\\in I_u} (r_{um} - \\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle - b_m) $$ where $I_u$ represents the set of movies rated by user $u$.\\n\\n##Reasons:\\n1. To find the optimal value of $b_u$, we differentiate the objective function with respect to $b_u$ and set the derivative to zero to find the critical point.\\n2. The derivative of the objective function with respect to $b_u$ in this case involves the sum over the movies rated by user $u$, denoted by $I_u$.\\n3. The optimal value of $b_u$ is determined by a weighted sum of the differences between the predicted score $\\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle + b_m$ and the actual ratings $r_{um}$ for the movies in $I_u$.\\n4. The term $\\\\lambda + |I_u|$ in the denominator serves as a regularization factor, adjusting the influence of the sum of differences on the optimal value of $b_u$ based on the number of ratings in $I_u$.',\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'B',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'B',\n",
       "   'other': ''}},\n",
       " {'A': '## Answer:\\nThe optimal value of $b_{u}$ is given by $$b_{u} = \\\\frac{\\\\sum_{m \\\\sim u} (r_{um} - \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle - b_{m})}{1 + \\\\lambda \\\\lVert \\\\mathbf{v}_{u} \\\\rVert^{2}}$$\\n\\n## Reasons:\\n1. In the matrix-factorization problem provided, the objective function includes terms related to the biases $b_{u}$ and $b_{m}$, as well as the user and movie vectors $\\\\mathbf{v}_{u}$ and $\\\\mathbf{w}_{m}$, respectively.\\n2. To find the optimal value of $b_{u}$ while keeping all other values fixed, we need to minimize the objective function with respect to $b_{u}$.\\n3. Taking the derivative of the objective function with respect to $b_{u}$ and setting it to zero gives us the optimal value of $b_{u}$.\\n4. The derivative of the objective function with respect to $b_{u}$ is $$\\\\frac{\\\\partial}{\\\\partial b_{u}} = \\\\sum_{m \\\\sim u} (f_{um} - r_{um}) - \\\\lambda b_{u}$$\\n5. Setting the derivative to zero and rearranging terms, we get the optimal value of $b_{u}$ as provided in the answer.\\n6. This optimal value accounts for the contributions of the observed ratings $r_{um}$, the user and movie vectors $\\\\mathbf{v}_{u}$ and $\\\\mathbf{w}_{m}$, and the biases $b_{m}$ while considering the regularization term controlled by $\\\\lambda$.\\n7. Computing this optimal value allows for updating the bias term $b_{u}$ in the matrix factorization model to minimize the objective function and improve the accuracy of the rating predictions.',\n",
       "  'B': '##Answer:\\nThe optimal value of $b_{u}$ is $\\\\frac{1}{|\\\\mathcal{R}_{u}|} \\\\sum_{m \\\\in \\\\mathcal{R}_{u}}\\\\left(r_{u m}-\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle-b_{m}\\\\right)$, where $\\\\mathcal{R}_{u}$ represents the set of movies rated by user $u$.\\n\\n##Reasons:\\nTo find the optimal value of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$ while keeping all other parameters fixed. This involves taking the derivative of the objective function with respect to $b_{u}$ and setting it to zero to find the minimum.\\n\\nGiven the objective function and the expression for $f_{u m}$, we can substitute $f_{u m}$ into the objective function. Then, taking the derivative of the objective function with respect to $b_{u}$ and setting it to zero will lead to the optimal value of $b_{u}$.\\n\\nThe optimal value of $b_{u}$ is obtained as $\\\\frac{1}{|\\\\mathcal{R}_{u}|} \\\\sum_{m \\\\in \\\\mathcal{R}_{u}}\\\\left(r_{u m}-\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle-b_{m}\\\\right)$, where $|\\\\mathcal{R}_{u}|$ denotes the number of movies rated by user $u$ and $\\\\mathcal{R}_{u}$ represents the set of movies rated by user $u$.\\n\\nTherefore, the optimal value of $b_{u}$ can be calculated using the given expression based on the ratings and the latent factor vectors for user $u$ and the movies rated by that user.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'A',\n",
       "   'relevance': 'A',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'A',\n",
       "   'other': ''}},\n",
       " {'A': 'To find the optimal value of $b_u$, we need to take the partial derivative of the objective function with respect to $b_u$ and set it equal to zero. However, since the bias terms $b_u$ are independent of each other, we can optimize them one at a time.\\n\\nTaking the derivative of the objective function with respect to $b_u$, we get:\\n\\n$\\\\frac{\\\\partial}{\\\\partial b_u} = \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_u$\\n\\nSetting this derivative to zero and solving for $b_u$, we get:\\n\\n$b_u = -\\\\frac{\\\\sum_{u \\\\sim m} (f_{um} - r_{um})}{\\\\lambda}$\\n\\nTherefore, the optimal value of $b_u$ is given by the negative sum of the errors $(f_{um} - r_{um})$ divided by $\\\\lambda$. This calculation can be repeated for each user $u$ to find the optimal bias term for that particular user.',\n",
       "  'B': 'Step 1: To find the optimal value of $b_u$, we minimize the objective function by differentiating with respect to $b_u$ and setting the derivative equal to zero.\\n\\nStep 2: The objective function is:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\n$$\\n\\nStep 3: Differentiating the objective function with respect to $b_u$ gives:\\n$$\\n\\\\frac{\\\\partial}{\\\\partial b_u} = \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) + \\\\lambda b_u\\n$$\\n\\nStep 4: Setting the derivative equal to zero and solving for $b_u$ yields:\\n$$\\nb_u = -\\\\frac{1}{\\\\lambda}\\\\sum_{u \\\\sim m} (f_{um} - r_{um})\\n$$\\n\\nTherefore, the optimal value of $b_u$ is $b_u = -\\\\frac{1}{\\\\lambda}\\\\sum_{u \\\\sim m} (f_{um} - r_{um})$.',\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'B',\n",
       "   'completeness': 'AB',\n",
       "   'other': ''}},\n",
       " {'A': 'To find the optimal value of $b_{u}$, we need to differentiate the objective function with respect to $b_{u}$ and set it to zero.\\n\\nTaking the derivative of the objective function with respect to $b_{u}$, we get:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_{u}} \\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}} (b_{u}^2 + ||\\\\mathbf{v}_{u}||^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_{m}^2 + ||\\\\mathbf{w}_{m}||^2)\\\\right]\\\\right)$$\\n\\nSince $b_{u}$ only appears in the first term of the objective function, the derivative simplifies to:\\n\\n$$\\\\sum_{m} (f_{um} - r_{um}) + \\\\lambda b_{u} = 0$$\\n\\nSolving for $b_{u}$, we get:\\n\\n$$b_{u} = -\\\\frac{\\\\sum_{m} (f_{um} - r_{um})}{\\\\lambda}$$\\n\\nTherefore, the optimal value of $b_{u}$ is given by $b_{u} = -\\\\frac{\\\\sum_{m} (f_{um} - r_{um})}{\\\\lambda}$, provided that all other values are fixed.',\n",
       "  'B': 'To find the optimal value of $b_u$, we need to take the derivative of the objective function with respect to $b_u$ and set it equal to zero.\\n\\nTaking the derivative of the objective function with respect to $b_u$, we get:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}(f_{um}-r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}(b_u^2 + \\\\|\\\\mathbf{v}_u\\\\|^2) + \\\\sum_{m \\\\in \\\\mathbf{M}}(b_m^2 + \\\\|\\\\mathbf{w}_m\\\\|^2)\\\\right]\\\\right)$$\\n\\nSimplifying this expression, we get:\\n\\n$$\\\\sum_{u \\\\sim m}(f_{um}-r_{um}) + \\\\lambda b_u = 0$$\\n\\nNow, plugging in the expression for $f_{um}$, we get:\\n\\n$$\\\\sum_{u \\\\sim m}(\\\\langle\\\\mathbf{v}_u, \\\\mathbf{w}_m\\\\rangle + b_u + b_m - r_{um}) + \\\\lambda b_u = 0$$\\n\\nSince we are optimizing with respect to $b_u$, we can isolate $b_u$ on one side:\\n\\n$$b_u = \\\\frac{1}{\\\\lambda} \\\\left(\\\\sum_{u \\\\sim m}(r_{um} - \\\\langle\\\\mathbf{v}_u, \\\\mathbf{w}_m\\\\rangle - b_m)\\\\right)$$\\n\\nTherefore, the optimal value of $b_u$ is given by the above expression.',\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'B',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'B',\n",
       "   'completeness': 'B',\n",
       "   'other': ''}},\n",
       " {'A': 'To find the optimal value of $b_u$ with all other values fixed, we can take the derivative of the objective function with respect to $b_u$ and set it equal to zero to find the minimum. \\n\\nTaking the derivative of the objective function with respect to $b_u$ and setting it equal to zero, we get:\\n\\n$$\\\\sum_{m \\\\in \\\\mathbf{M}}(f_{um} - r_{um}) + \\\\lambda b_u = 0$$\\n\\nSubstitute the expression for $f_{um}$ into the equation:\\n\\n$$\\\\sum_{m \\\\in \\\\mathbf{M}}((\\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle + b_u + b_m) - r_{um}) + \\\\lambda b_u = 0$$\\n\\nRearranging terms, we get:\\n\\n$$\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle + \\\\sum_{m \\\\in \\\\mathbf{M}}b_u + \\\\sum_{m \\\\in \\\\mathbf{M}}b_m - \\\\sum_{m \\\\in \\\\mathbf{M}}r_{um} + \\\\lambda b_u = 0$$\\n\\nSince $\\\\mathbf{v}_u$ and $\\\\mathbf{w}_m$ are fixed, the first term does not depend on $b_u$. The second term becomes $\\\\sum_{m \\\\in \\\\mathbf{M}}b_u = |M| \\\\cdot b_u$ where $|M|$ denotes the number of movies. The third term becomes $\\\\sum_{m \\\\in \\\\mathbf{M}}b_m$ which is a constant and does not depend on $b_u$. The fourth term is the sum of the observed ratings for user $u$.\\n\\nTherefore, the optimal value of $b_u$ can be computed as:\\n\\n$$b_u = \\\\frac{1}{|M| + \\\\lambda} \\\\left(\\\\sum_{m \\\\in \\\\mathbf{M}}(r_{um} - \\\\sum_{m \\\\in \\\\mathbf{M}} b_m)\\\\right)$$\\n\\nThis formula provides the optimal value of $b_u$ given that all other parameters are fixed.',\n",
       "  'B': \"To find the optimal value of $b_u$ given that all other values are fixed, we need to minimize the objective function with respect to $b_u$. \\n\\nLet's differentiate the objective function with respect to $b_u$ and set the derivative to zero to find the optimal value of $b_u$.\\n\\nThe objective function is:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\n$$\\n\\nThe term involving $b_u$ is:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right)^{2} + \\\\frac{\\\\lambda}{2}\\\\left[b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right]\\n$$\\n\\nDifferentiating with respect to $b_u$ and setting the derivative to zero:\\n$$\\n\\\\sum_{u \\\\sim m}\\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right) + \\\\lambda b_u = 0\\n$$\\n\\nRearranging terms, we get:\\n$$\\n\\\\sum_{u \\\\sim m} \\\\left(f_{um} - r_{um}\\\\right) + \\\\lambda b_u = 0\\n$$\\n\\nFinally, the optimal value of $b_u$ is:\\n$$\\nb_u = -\\\\frac{1}{\\\\lambda}\\\\sum_{u \\\\sim m} \\\\left(f_{um} - r_{um}\\\\right)\\n$$\\n\\nTherefore, the optimal value of $b_u$ is the negative of the average error between the estimated score $f_{um}$ and the actual rating $r_{um}$, divided by $\\\\lambda$.\",\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'A',\n",
       "   'relevance': 'A',\n",
       "   'clarity': 'A',\n",
       "   'completeness': 'A',\n",
       "   'other': ''}},\n",
       " {'A': 'Answer: $b_u = \\\\frac{1}{|R_u|} \\\\sum_{m \\\\in R_u} (r_{um} - \\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\\\rangle - b_m)$\\n\\nExplanation: The optimal value of $b_u$ can be calculated by taking the derivative of the objective function with respect to $b_u$ and setting it to zero. By doing so, we obtain the expression provided in the answer. This expression represents the average of the differences between the observed ratings and the estimated scores for all movies rated by user $u$, adjusted by the bias term $b_m$. This calculation ensures that the bias term $b_u$ is optimized to minimize the overall objective function.',\n",
       "  'B': 'Answer: $b_{u}=\\\\frac{1}{\\\\lambda}\\\\sum_{m \\\\sim u}\\\\left(r_{u m}-\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle-b_{m}\\\\right)$\\n\\nExplanation: To find the optimal value of $b_{u}$, we differentiate the objective function with respect to $b_{u}$ and set it to zero. This gives us the expression for $b_{u}$ as shown above. This expression represents the weighted average of the differences between the observed rating and the predicted rating for user $u$ across all movies $m$ that user $u$ has rated. By setting the derivative to zero, we are essentially minimizing the objective function with respect to $b_{u}$.\\n\\nAlternative interpretations such as directly setting the derivative to zero without considering the weighted average would not be correct as they do not take into account the importance of each rating in the optimization process. The weighted average considers the contribution of each rating based on the difference between observed and predicted values.',\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'B',\n",
       "   'relevance': 'B',\n",
       "   'clarity': 'B',\n",
       "   'completeness': 'B',\n",
       "   'other': ''}},\n",
       " {'A': 'The optimal value of $b_u$ is obtained by setting the derivative of the objective function with respect to $b_u$ to zero. The correct formula for $b_u$ is: $$b_u = \\x0crac{\\\\sum_{m \\\\sim u} (r_{um} - \\\\langle \\\\mathbf{v}_u, \\\\mathbf{w}_m \\rangle - b_m)}{\\\\lambda + |m \\\\sim u|}$$ This formula takes into account the regularization term $\\\\lambda$ and the number of ratings for user $u$ denoted by $|m \\\\sim u|$. Dividing by $\\\\lambda + |m \\\\sim u|$ ensures that the bias term $b_u$ is updated appropriately based on the available ratings for user $u$ and the regularization parameter $\\\\lambda$. This calculation allows for better optimization of the matrix factorization model in the context of collaborative filtering.',\n",
       "  'B': 'The optimal values of $b_{u}$ are given by:$$b_{u} = \\x0crac{\\\\sum_{m \\\\sim u} (r_{um} - \\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\rangle - b_{m})}{\\\\lambda}$$ However, this answer is incorrect. The correct calculation involves dividing by $\\\\lambda + |m \\\\sim u|$ instead of just $\\\\lambda$.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'clarity': 'None',\n",
       "   'completeness': 'A',\n",
       "   'correctness': 'A',\n",
       "   'other': 'Conciseness: B; Engagement: AB',\n",
       "   'overall': 'A',\n",
       "   'relevance': 'A'}},\n",
       " {'A': \"To find the optimal value of $b_u$ while keeping all other values fixed, we need to minimize the objective function with respect to $b_u$. Let's first rewrite the objective function in a more explicit form:\\n\\n$$\\nL = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m})^2 + \\\\frac{\\\\lambda}{2} \\\\left( \\\\sum_{u \\\\in \\\\mathbf{U}} \\\\left(b_u^2 + \\\\|\\\\mathbf{v}_u\\\\|^2\\\\right) + \\\\sum_{m \\\\in \\\\mathbf{M}} \\\\left(b_m^2 + \\\\|\\\\mathbf{w}_m\\\\|^2\\\\right)\\\\right)\\n$$\\n\\nExpanding the first term in the objective function:\\n\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m})^2 = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} \\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right)^{2}\\n$$\\n\\nDifferentiating the objective function $L$ with respect to $b_u$ gives:\\n\\n$$\\n\\\\frac{\\\\partial L}{\\\\partial b_u} = \\\\sum_{u \\\\sim m} (f_{u m} - r_{u m}) + \\\\lambda b_u = 0\\n$$\\n\\nSubstitute the expression for $f_{u m}$ into the above equation:\\n\\n$$\\n\\\\sum_{u \\\\sim m} \\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right) + \\\\lambda b_u = 0\\n$$\\n\\nSince we are solving for the optimal value of $b_u$, we can isolate $b_u$ on one side:\\n\\n$$\\nb_u = -\\\\frac{1}{\\\\lambda} \\\\left(\\\\sum_{u \\\\sim m} \\\\left(\\\\left\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\right\\\\rangle + b_m - r_{u m}\\\\right)\\\\right)\\n$$\\n\\nTherefore, the optimal value of $b_u$ is given by the negative average of the differences between the predicted rating and the actual rating for each movie $m$, weighted by the regularization parameter $\\\\lambda$.\",\n",
       "  'B': \"To find the optimal value of $b_u$ with all other values fixed, we need to minimize the objective function with respect to $b_u$. \\n\\nGiven the objective function:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\n$$\\n\\nWe can see that the term involving $b_u$ is:\\n$$\\n\\\\frac{1}{2}\\\\sum_{m \\\\in \\\\mathbf{M}} (f_{u m} - r_{u m})^2 + \\\\frac{\\\\lambda}{2} \\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\n$$\\n\\nTo find the optimal value for $b_u$, we need to take the derivative of this term with respect to $b_u$ and set it to zero:\\n$$\\n\\\\frac{\\\\partial}{\\\\partial b_u} \\\\left(\\\\frac{1}{2}\\\\sum_{m \\\\in \\\\mathbf{M}} (f_{u m} - r_{u m})^2 + \\\\frac{\\\\lambda}{2} \\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\\\right) = 0\\n$$\\n\\nExpanding this derivative and setting it to zero will give us the optimal value for $b_u$. Let's proceed with the calculations.\",\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'AB',\n",
       "   'other': 'Conciseness: B; Engagement: B'}},\n",
       " {'A': '1. Matrix-factorization problem: In the context of machine learning, matrix factorization is a technique used for collaborative filtering. It decomposes a matrix of user-item interactions into the product of two lower-dimensional matrices to discover hidden patterns or features.\\n\\n2. Observed ratings $r_{u m}$: Refers to the actual ratings given by user $u$ to movie $m$. These ratings are the ground truth values that the model aims to predict.\\n\\n3. Estimate the score $f_{u m}$: The score is estimated using the inner product of the latent factor vectors of user $u$ ($\\\\mathbf{v}_{u}$) and movie $m$ ($\\\\mathbf{w}_{m}$), along with user bias $b_{u}$ and movie bias $b_{m}$.\\n\\n4. Objective function: The objective function represents the loss function that the model aims to minimize. It consists of two terms - the squared error term measuring the difference between predicted and observed ratings, and a regularization term to prevent overfitting by penalizing large values of biases and latent factor vectors.\\n\\n5. Optimal values of $b_{u}$: To find the optimal value of $b_{u}$ while keeping all other values fixed, we need to take the derivative of the objective function with respect to $b_{u}$, set it to zero, and solve for $b_{u}$.\\n\\n6. To find the optimal value of $b_{u}$, differentiate the objective function with respect to $b_{u$ and set it to zero:\\n\\n$$\\\\frac{\\\\partial}{\\\\partial b_{u}}\\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}(f_{u m}-r_{u m})^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}(b_{u}^{2}+\\\\|\\\\mathbf{v}_{u}\\\\|^{2})+\\\\sum_{m \\\\in \\\\mathbf{M}}(b_{m}^{2}+\\\\|\\\\mathbf{w}_{m}\\\\|^{2})\\\\right]\\\\right) = 0$$\\n\\n7. Solve for $b_{u}$ by setting the derivative to zero and rearranging terms accordingly.\\n\\nBy solving the derived equation, you can find the optimal value of $b_{u}$ that minimizes the objective function. This process ensures that the bias term $b_{u}$ is adjusted to best fit the observed ratings while considering regularization to prevent overfitting.',\n",
       "  'B': 'To find the optimal value of \\\\( b_u \\\\) provided that all other values are fixed, we can differentiate the objective function with respect to \\\\( b_u \\\\) and set the derivative to zero. \\n\\nDifferentiating the objective function with respect to \\\\( b_u \\\\), we get:\\n\\n\\\\[ \\\\frac{\\\\partial}{\\\\partial b_u} \\\\left( \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\left[ \\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2 + ||\\\\mathbf{v}_u||^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_m^2 + ||\\\\mathbf{w}_m||^2) \\\\right] \\\\right) = 0 \\\\]\\n\\nSolving this equation will give us the optimal value of \\\\( b_u \\\\) in terms of the other fixed values in the problem.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'AB',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'A',\n",
       "   'other': ''}},\n",
       " {'A': \"To find the optimal value of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$. \\n\\nLet's start by expanding $f_{um}$:\\n$$\\nf_{um} = \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m}\\n$$\\n\\nNow, substitute $f_{um}$ into the objective function:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{um} \\\\right)^{2} + \\\\frac{\\\\lambda}{2} \\\\left[ \\\\sum_{u} \\\\left( b_{u}^{2} + \\\\left\\\\| \\\\mathbf{v}_{u} \\\\right\\\\|^{2} \\\\right) + \\\\sum_{m} \\\\left( b_{m}^{2} + \\\\left\\\\| \\\\mathbf{w}_{m} \\\\right\\\\|^{2} \\\\right) \\\\right]\\n$$\\n\\nWe are interested in the terms that contain $b_{u}$, so let's focus on those. Taking the derivative of the objective function with respect to $b_{u}$, we get:\\n$$\\n\\\\frac{\\\\partial}{\\\\partial b_{u}} = \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{um} \\\\right) + \\\\lambda b_{u}\\n$$\\n\\nSetting this derivative to 0 and solving for $b_{u}$, we have:\\n$$\\n\\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{um} \\\\right) + \\\\lambda b_{u} = 0\\n$$\\n\\nSimplify this equation:\\n$$\\n\\\\sum_{u \\\\sim m} \\\\left( b_{u} \\\\right) + \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{m} - r_{um} \\\\right) + \\\\lambda b_{u} = 0\\n$$\\n\\nFactor out $b_{u}$ from the sum:\\n$$\\n\\\\left( \\\\sum_{u \\\\sim m} 1 \\\\right) b_{u} + \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle - r_{um} \\\\right) + \\\\lambda b_{u} = 0\\n$$\\n\\nSimplify and solve for $b_{u}$:\\n$$\\nb_{u} = -\\\\frac{\\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle - r_{um} \\\\right)}{\\\\sum_{u \\\\sim m} 1 + \\\\lambda}\\n$$\\n\\nThis is the optimal value of $b_{u}$ provided that all other values are fixed.\",\n",
       "  'B': \"To find the optimal values of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$ while keeping all other variables fixed.\\n\\nFirst, let's expand the term $f_{um}$:\\n\\n$$ f_{um} = \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} $$\\n\\nSubstitute $f_{um}$ back into the objective function:\\n\\n$$ \\\\frac{1}{2} \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{um} \\\\right)^{2} + \\\\frac{\\\\lambda}{2} \\\\left[ \\\\sum_{u \\\\in \\\\mathbf{U}} \\\\left( b_{u}^{2} + \\\\left\\\\| \\\\mathbf{v}_{u} \\\\right\\\\|^{2} \\\\right) + \\\\sum_{m \\\\in \\\\mathbf{M}} \\\\left( b_{m}^{2} + \\\\left\\\\| \\\\mathbf{w}_{m} \\\\right\\\\|^{2} \\\\right) \\\\right] $$\\n\\nNow, we need to focus on the terms that contain $b_{u}$ and differentiate with respect to $b_{u}$:\\n\\n$$ \\\\frac{\\\\partial}{\\\\partial b_{u}} = \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle + b_{u} + b_{m} - r_{um} \\\\right) + \\\\lambda b_{u} = 0 $$\\n\\nSeparating the terms containing $b_{u}$, we get:\\n\\n$$ \\\\sum_{u \\\\sim m} \\\\left( b_{u} \\\\right) + |u \\\\sim m| b_{u} + \\\\lambda b_{u} = 0 $$\\n\\n$$ \\\\left( |u \\\\sim m| + \\\\lambda \\\\right) b_{u} = - \\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle - r_{um} - b_{m} \\\\right) $$\\n\\nTherefore, the optimal value of $b_{u}$ is given by:\\n\\n$$ b_{u} = - \\\\frac{\\\\sum_{u \\\\sim m} \\\\left( \\\\left\\\\langle \\\\mathbf{v}_{u}, \\\\mathbf{w}_{m} \\\\right\\\\rangle - r_{um} - b_{m} \\\\right)}{|u \\\\sim m| + \\\\lambda} $$\\n\\nThis is the optimal value of $b_{u}$ provided that all other values are fixed.\",\n",
       "  'overall': 'A',\n",
       "  'criteria': {'overall': 'A',\n",
       "   'correctness': 'A',\n",
       "   'relevance': 'A',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'A',\n",
       "   'other': 'Conciseness: A; Engagement: A'}},\n",
       " {'A': \"To find the optimal value of $b_u$, we need to first consider the objective function and simplify it in terms of $b_u$. If we take the derivative of the objective function with respect to $b_u$ and set it equal to zero, we'll find the optimal value of $b_u$.\\n\\nGiven the objective function\\n$$\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right],$$\\nwe can simplify it in terms of $b_u$ by focusing on the terms that involve $b_u$:\\n$$\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right).$$\\n\\nTaking the derivative with respect to $b_u$ and setting it to zero, we get:\\n$$\\\\frac{d}{d b_u}\\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\\\right) = 0.$$\\n\\nSince the terms in the sum do not depend on $b_u$ except for those with $b_u$, we can focus on the term involving $b_u$:\\n$$\\\\frac{d}{d b_u}\\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\\\right) = 0.$$\\n\\nExpanding this, we get:\\n$$\\\\frac{d}{d b_u}\\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2} + \\\\frac{\\\\lambda}{2}b_{u}^{2} + \\\\frac{\\\\lambda}{2}\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right) = 0.$$\\n\\nDifferentiating each term with respect to $b_u$, we find:\\n$$\\\\frac{d}{d b_u}\\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}\\\\right) + \\\\frac{d}{d b_u}\\\\left(\\\\frac{\\\\lambda}{2}b_{u}^{2}\\\\right) + \\\\frac{d}{d b_u}\\\\left(\\\\frac{\\\\lambda}{2}\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right) = 0.$$\\n\\nSimplifying further:\\n$$\\\\lambda b_u + 0 + 0 = 0.$$\\n\\nTherefore, the optimal value of $b_u$ is $0$.\",\n",
       "  'B': \"To find the optimal value of $b_u$, we need to minimize the objective function with respect to $b_u$. Let's denote the objective function as $L$:\\n\\n$$ L = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2 + ||\\\\mathbf{v}_u||^2) + \\\\sum_{m \\\\in \\\\mathbf{M}} (b_m^2 + ||\\\\mathbf{w}_m||^2)\\\\right] $$\\n\\nThe part of the objective function that involves $b_u$ is:\\n\\n$$ L_{b_u} = \\\\frac{1}{2} \\\\sum_{u \\\\sim m} (f_{um} - r_{um})^2 + \\\\frac{\\\\lambda}{2} \\\\sum_{u \\\\in \\\\mathbf{U}} (b_u^2 + ||\\\\mathbf{v}_u||^2) $$\\n\\nTo minimize $L_{b_u}$, we take the derivative with respect to $b_u$ and set it to zero:\\n\\n$$ \\\\frac{\\\\partial L_{b_u}}{\\\\partial b_u} = \\\\lambda b_u + \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) = 0 $$\\n\\nSolving for $b_u$ gives:\\n\\n$$ b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) $$\\n\\nTherefore, the optimal value of $b_u$ is given by:\\n\\n$$ b_u = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m} (f_{um} - r_{um}) $$\\n\\nThis formula provides the optimal bias value $b_u$ based on the given matrix-factorization problem and objective function.\",\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'B',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'AB',\n",
       "   'completeness': 'AB',\n",
       "   'other': 'Conciseness: B; Engagement: A'}},\n",
       " {'A': 'To find the optimal value of $b_u$, we can take the derivative of the objective function with respect to $b_u$ and set it equal to zero. \\\\n\\\\nTaking the derivative of the objective function with respect to $b_u$ gives:\\\\n\\\\n$$ \\\\\\\\sum_{m \\\\\\\\sim u} (f_{um} - r_{um}) + \\\\\\\\lambda b_u = 0 $$\\\\n\\\\nSetting this equal to zero and solving for $b_u$ gives:\\\\n\\\\n$$ b_u = \\\\\\\\frac{1}{\\\\\\\\lambda} \\\\\\\\left( r_u - \\\\\\\\sum_{m \\\\\\\\sim u} \\\\\\\\left( \\\\\\\\left\\\\\\\\langle \\\\\\\\mathbf{v}_u, \\\\\\\\mathbf{w}_m \\\\\\\\right\\\\\\\\rangle + b_m \\\\\\\\right) \\\\\\\\right) $$\\\\n\\\\nSo, the optimal value of $b_u$ is given by the above equation.',\n",
       "  'B': \"Let's include the term with ${b_{u}}^2$ in our derivative calculation. The derivative of the objective function with respect to $b_{u}$ is:\\\\n\\\\n$$\\\\n\\\\\\\\frac{\\\\\\\\partial}{\\\\\\\\partial b_{u}}\\\\\\\\left[\\\\\\\\frac{1}{2} \\\\\\\\sum_{u \\\\\\\\sim m}\\\\\\\\left(f_{u m}-r_{u m}\\\\\\\\right)^{2}+\\\\\\\\frac{\\\\\\\\lambda}{2}\\\\\\\\left[\\\\\\\\sum_{u \\\\\\\\in \\\\\\\\mathbf{U}}\\\\\\\\left(b_{u}^{2}+\\\\\\\\left\\\\\\\\|\\\\\\\\mathbf{v}_{u}\\\\\\\\right\\\\\\\\|^{2}\\\\\\\\right)+\\\\\\\\sum_{m \\\\\\\\in \\\\\\\\mathbf{M}}\\\\\\\\left(b_{m}^{2}+\\\\\\\\left\\\\\\\\|\\\\\\\\mathbf{w}_{m}\\\\\\\\right\\\\\\\\|^{2}\\\\\\\\right)\\\\\\\\right]\\\\\\\\right]=0\\\\n$$\\\\n\\\\nExpanding the first term in the objective function, we get:\\\\n\\\\n$$\\\\n\\\\\\\\frac{\\\\\\\\partial}{\\\\\\\\partial b_{u}}\\\\\\\\left[\\\\\\\\frac{1}{2} \\\\\\\\sum_{u \\\\\\\\sim m}\\\\\\\\left(\\\\\\\\left\\\\\\\\langle\\\\\\\\mathbf{v}_{u}, \\\\\\\\mathbf{w}_{m}\\\\\\\\right\\\\\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\\\\\right)^{2}\\\\\\\\right]=0\\\\n$$\\\\n\\\\nTaking the derivative of this expression with respect to $b_{u}$, we get:\\\\n\\\\n$$\\\\n\\\\\\\\sum_{u \\\\\\\\sim m}\\\\\\\\left(\\\\\\\\left\\\\\\\\langle\\\\\\\\mathbf{v}_{u}, \\\\\\\\mathbf{w}_{m}\\\\\\\\right\\\\\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\\\\\right) + \\\\\\\\lambda b_{u} = 0\\\\n$$\\\\n\\\\nIsolating $b_{u}$ in this equation, we get:\\\\n\\\\n$$\\\\nb_{u} = \\\\\\\\frac{\\\\\\\\sum_{u \\\\\\\\sim m}\\\\\\\\left(r_{u m} - \\\\\\\\left\\\\\\\\langle\\\\\\\\mathbf{v}_{u}, \\\\\\\\mathbf{w}_{m}\\\\\\\\right\\\\\\\\rangle - b_{m}\\\\\\\\right)}{\\\\\\\\lambda}\\\\n$$\\\\n\\\\nThis provides the optimal value of $b_{u}$ given that all other values are fixed, including the term with ${b_{u}}^2$.\",\n",
       "  'overall': 'B',\n",
       "  'criteria': {'overall': 'B',\n",
       "   'correctness': 'None',\n",
       "   'relevance': 'AB',\n",
       "   'clarity': 'None',\n",
       "   'completeness': 'B',\n",
       "   'other': 'Conciseness: AB; Engagement: B'}},\n",
       " {'A': \"To find the optimal value of $b_{u}$, we need to take the derivative of the objective function with respect to $b_{u}$ and set it to zero.\\n\\nGiven the objective function:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\n$$\\n\\nLet's consider the term involving $b_{u}$:\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2} + \\\\frac{\\\\lambda}{2}\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\n$$\\n\\nTaking the derivative with respect to $b_{u}$ and setting it to zero:\\n$$\\n\\\\frac{\\\\partial}{\\\\partial b_{u}}\\\\left(\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2} + \\\\frac{\\\\lambda}{2}\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)\\\\right) = 0\\n$$\\n\\nExpanding the derivative term with $b_{u}$:\\n$$\\n\\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right) + \\\\lambda b_{u} = 0\\n$$\\n\\nRearranging the terms to solve for $b_{u}$:\\n$$\\nb_{u} = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)\\n$$\\n\\nSo, the optimal value of $b_{u}$ is given by $b_{u} = -\\\\frac{1}{\\\\lambda} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)$.\",\n",
       "  'B': 'To find the optimal value of $b_{u}$, we need to minimize the objective function with respect to $b_{u}$ while keeping all other variables fixed.\\n\\nGiven the objective function:\\n\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(f_{u m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[\\\\sum_{u \\\\in \\\\mathbf{U}}\\\\left(b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right)+\\\\sum_{m \\\\in \\\\mathbf{M}}\\\\left(b_{m}^{2}+\\\\left\\\\|\\\\mathbf{w}_{m}\\\\right\\\\|^{2}\\\\right)\\\\right]\\n$$\\n\\nWe can see that the term involving $b_{u}$ is:\\n\\n$$\\n\\\\frac{1}{2} \\\\sum_{u \\\\sim m}\\\\left(\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\rangle+b_{u}+b_{m}-r_{u m}\\\\right)^{2}+\\\\frac{\\\\lambda}{2}\\\\left[b_{u}^{2}+\\\\left\\\\|\\\\mathbf{v}_{u}\\\\right\\\\|^{2}\\\\right]\\n$$\\n\\nTo minimize this term with respect to $b_{u}$, we need to take the derivative with respect to $b_{u}$ and set it to zero:\\n\\n$$\\n\\\\frac{\\\\partial}{\\\\partial b_{u}} = \\\\sum_{u \\\\sim m}(\\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\rangle+b_{u}+b_{m}-r_{u m}) + \\\\lambda b_{u} = 0\\n$$\\n\\nSolving for $b_{u}$:\\n\\n$$\\nb_{u} = \\\\frac{\\\\sum_{u \\\\sim m}(r_{u m} - \\\\langle\\\\mathbf{v}_{u}, \\\\mathbf{w}_{m}\\\\rangle - b_{m})}{\\\\lambda}\\n$$\\n\\nTherefore, the optimal value of $b_{u}$ is given by the above expression, provided that all other values are fixed.',\n",
       "  'overall': 'A',\n",
       "  'criteria': {'correctness': 'A',\n",
       "   'relevance': 'A',\n",
       "   'clarity': 'A',\n",
       "   'completeness': 'A',\n",
       "   'other': 'None',\n",
       "   'overall': 'A'}}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['preference']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1522 questions, and for each question we have N preference pairs (N is not always the same). We will extract the preference pairs from this data according to the preference dictionnaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting single preference dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to expand the dataframe so that each row has only one preference dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_complete</th>\n",
       "      <th>course_id</th>\n",
       "      <th>preference_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Question: Consider the following contains func...</td>\n",
       "      <td>15000</td>\n",
       "      <td>{'A': 'The asymptotic depth of the contains fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Question: Consider the following contains func...</td>\n",
       "      <td>15000</td>\n",
       "      <td>{'A': 'The asymptotic depth of the contains fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Question: Consider the following contains func...</td>\n",
       "      <td>15000</td>\n",
       "      <td>{'A': 'The asymptotic depth of the `contains` ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Question: Consider the following contains func...</td>\n",
       "      <td>15000</td>\n",
       "      <td>{'A': 'To determine the asymptotic depth of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Question: Consider the following contains func...</td>\n",
       "      <td>15000</td>\n",
       "      <td>{'A': 'The asymptotic depth of the contains fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                  question_complete  course_id  \\\n",
       "0            0  Question: Consider the following contains func...      15000   \n",
       "0            0  Question: Consider the following contains func...      15000   \n",
       "0            0  Question: Consider the following contains func...      15000   \n",
       "0            0  Question: Consider the following contains func...      15000   \n",
       "0            0  Question: Consider the following contains func...      15000   \n",
       "\n",
       "                                     preference_dict  \n",
       "0  {'A': 'The asymptotic depth of the contains fu...  \n",
       "0  {'A': 'The asymptotic depth of the contains fu...  \n",
       "0  {'A': 'The asymptotic depth of the `contains` ...  \n",
       "0  {'A': 'To determine the asymptotic depth of th...  \n",
       "0  {'A': 'The asymptotic depth of the contains fu...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_rows = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for pref in row['preference']:\n",
    "        new_row = row.copy()\n",
    "        new_row['preference_dict'] = pref\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df.drop(columns=['preference'], inplace=True)\n",
    "\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26738"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us 26k disctinct preference pairs from the 1522 questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows starting with 'Question: ': 26738\n",
      "Number of rows not starting with 'Question: ': 0\n"
     ]
    }
   ],
   "source": [
    "# Check if each row in 'question_complete' starts with same prefix 'Question: '\n",
    "prefix_check = expanded_df['question_complete'].apply(lambda x: x.startswith('Question: '))\n",
    "\n",
    "print(\"Number of rows starting with 'Question: ':\", prefix_check.sum())\n",
    "print(\"Number of rows not starting with 'Question: ':\", len(prefix_check) - prefix_check.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the prefix 'Question: ' from the question prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['question_complete'] = expanded_df['question_complete'].apply(lambda x: x[10:])\n",
    "\n",
    "expanded_df.rename(columns={'question_complete': 'prompt'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Consider the following contains function defin...\n",
       "0       Consider the following contains function defin...\n",
       "0       Consider the following contains function defin...\n",
       "0       Consider the following contains function defin...\n",
       "0       Consider the following contains function defin...\n",
       "                              ...                        \n",
       "1521    Consider the transformation from binary MRSW s...\n",
       "1521    Consider the transformation from binary MRSW s...\n",
       "1521    Consider the transformation from binary MRSW s...\n",
       "1521    Consider the transformation from binary MRSW s...\n",
       "1521    Consider the transformation from binary MRSW s...\n",
       "Name: prompt, Length: 26738, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to HF documentation, prompts for phi should be formatted like this:\n",
    "\n",
    "`<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "Question?<|end|>\n",
    "<|assistant|>`\n",
    "\n",
    "We will format our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(row):\n",
    "    formatted_prompt = (\n",
    "        f\"<|system|>\"\n",
    "        f\"You are a helpful assistant for a STEM student.\"\n",
    "        f\"<|end|>\"\n",
    "        f\"<|user|>{row['prompt']}<|end|>\"\n",
    "        f\"<|assistant|>\"\n",
    "    )\n",
    "    return formatted_prompt\n",
    "\n",
    "expanded_df['prompt'] = expanded_df.apply(format_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting preference pairs\n",
    "For each row, we set `'chosen'` as the answer defined as best overall in the preference dict, and `'rejected'` as the other answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preferences(row):\n",
    "    overall = row['preference_dict'].get('overall')\n",
    "    if overall not in ['A', 'B']:\n",
    "        return None\n",
    "    elif overall == 'A':\n",
    "        chosen = row['preference_dict'].get('A')\n",
    "        rejected = row['preference_dict'].get('B')\n",
    "    else:\n",
    "        chosen = row['preference_dict'].get('B')\n",
    "        rejected = row['preference_dict'].get('A')\n",
    "    \n",
    "    return pd.Series({'chosen': chosen, 'rejected': rejected})\n",
    "\n",
    "preferences_df = expanded_df.apply(extract_preferences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([expanded_df, preferences_df], axis=1)\n",
    "final_df = final_df[['prompt', 'chosen', 'rejected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>When `contains` is called on a List, the `drop...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The asymptotic depth of the `contains` functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The contains function is a recursive function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "      <td>When the contains function is called on a List...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  When `contains` is called on a List, the `drop...   \n",
       "0  To determine the asymptotic depth of the `cont...   \n",
       "0  To determine the asymptotic depth of the `cont...   \n",
       "0  To determine the asymptotic depth of the `cont...   \n",
       "0  The asymptotic depth of the contains function ...   \n",
       "\n",
       "                                            rejected  \n",
       "0  The asymptotic depth of the contains function ...  \n",
       "0  The asymptotic depth of the contains function ...  \n",
       "0  The asymptotic depth of the `contains` functio...  \n",
       "0  The contains function is a recursive function ...  \n",
       "0  When the contains function is called on a List...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. M1 preference data filtering\n",
    "Our data is now in the correct format, we will filter out pairs where:\n",
    "- `chosen` and `rejected` are the same\n",
    "- length of prompt or answer is too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26498"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df[final_df['chosen'] != final_df['rejected']]\n",
    "\n",
    "# We lost a few rows that would have been irrelevant:\n",
    "len(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['prompt_len'] = final_df['prompt'].apply(len)\n",
    "final_df['c_len'] = final_df['chosen'].apply(len)\n",
    "final_df['r_len'] = final_df['rejected'].apply(len)\n",
    "final_df['target_len'] = final_df[['c_len', 'r_len']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_hist(col_name, title):\n",
    "    threshold = np.percentile(final_df[col_name], 95)\n",
    "\n",
    "    # Histogram:\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.hist(final_df[col_name], bins=30, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'95th percentile ({threshold:.2f})')\n",
    "\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Length Repartition of '+title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_hist('prompt_len', 'Prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_hist('target_len', 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in ['prompt_len', 'target_len']:\n",
    "    threshold = np.percentile(final_df[col_name], 95)\n",
    "    final_df = final_df[final_df[col_name] <= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23933\n",
      "1310\n",
      "2361\n"
     ]
    }
   ],
   "source": [
    "print(len(final_df))\n",
    "print(final_df.prompt_len.max())\n",
    "print(final_df.target_len.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>When `contains` is called on a List, the `drop...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The asymptotic depth of the `contains` functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The contains function is a recursive function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "      <td>When the contains function is called on a List...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  When `contains` is called on a List, the `drop...   \n",
       "0  To determine the asymptotic depth of the `cont...   \n",
       "0  To determine the asymptotic depth of the `cont...   \n",
       "0  To determine the asymptotic depth of the `cont...   \n",
       "0  The asymptotic depth of the contains function ...   \n",
       "\n",
       "                                            rejected  \n",
       "0  The asymptotic depth of the contains function ...  \n",
       "0  The asymptotic depth of the contains function ...  \n",
       "0  The asymptotic depth of the `contains` functio...  \n",
       "0  The contains function is a recursive function ...  \n",
       "0  When the contains function is called on a List...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df[['prompt', 'chosen', 'rejected']]\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(read_jsonl('DPO_data.jsonl'))\n",
    "\n",
    "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "train_df.to_json('DPO_train.jsonl', orient='records', lines=True)\n",
    "eval_df.to_json('DPO_eval.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End: Save as jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_json('DPO_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(read_jsonl('DPO_data.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>When `contains` is called on a List, the `drop...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The asymptotic depth of the `contains` functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>To determine the asymptotic depth of the `cont...</td>\n",
       "      <td>The contains function is a recursive function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|system|&gt;You are a helpful assistant for a ST...</td>\n",
       "      <td>The asymptotic depth of the contains function ...</td>\n",
       "      <td>When the contains function is called on a List...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  <|system|>You are a helpful assistant for a ST...   \n",
       "1  <|system|>You are a helpful assistant for a ST...   \n",
       "2  <|system|>You are a helpful assistant for a ST...   \n",
       "3  <|system|>You are a helpful assistant for a ST...   \n",
       "4  <|system|>You are a helpful assistant for a ST...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  When `contains` is called on a List, the `drop...   \n",
       "1  To determine the asymptotic depth of the `cont...   \n",
       "2  To determine the asymptotic depth of the `cont...   \n",
       "3  To determine the asymptotic depth of the `cont...   \n",
       "4  The asymptotic depth of the contains function ...   \n",
       "\n",
       "                                            rejected  \n",
       "0  The asymptotic depth of the contains function ...  \n",
       "1  The asymptotic depth of the contains function ...  \n",
       "2  The asymptotic depth of the `contains` functio...  \n",
       "3  The contains function is a recursive function ...  \n",
       "4  When the contains function is called on a List...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23933"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
