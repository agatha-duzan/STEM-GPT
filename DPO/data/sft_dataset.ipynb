{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Make sure transformers is up to date!\n",
    "\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolas/anaconda3/envs/mnlp-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset,concatenate_datasets,DatasetDict,load_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "List 3 synonyms for the word \"tiny\"<|endoftext|>\n",
      "<|assistant|>\n",
      "1. tomity \n",
      "2. miniaturity \n",
      "3. petiteness<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('stabilityai/stablelm-zephyr-3b')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'stabilityai/stablelm-zephyr-3b',\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test from huggingface's website:\n",
    "prompt = [{'role': 'user', 'content': 'List 3 synonyms for the word \"tiny\"'}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokens = model.generate(\n",
    "    inputs.to(model.device),\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.8,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. math_dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dataset_options = ['algebra__linear_1d', 'algebra__linear_1d_composed', 'algebra__linear_2d', 'algebra__linear_2d_composed', 'algebra__polynomial_roots', 'algebra__polynomial_roots_composed', 'algebra__sequence_next_term', 'algebra__sequence_nth_term', 'arithmetic__add_or_sub','arithmetic__add_or_sub_in_base', 'arithmetic__add_sub_multiple', 'arithmetic__div', 'arithmetic__mixed', 'arithmetic__mul', 'arithmetic__mul_div_multiple', 'arithmetic__nearest_integer_root', 'arithmetic__simplify_surd', 'calculus__differentiate', 'calculus__differentiate_composed', 'comparison__closest', 'comparison__closest_composed', 'comparison__kth_biggest', 'comparison__kth_biggest_composed', 'comparison__pair', 'comparison__pair_composed', 'comparison__sort', 'comparison__sort_composed', 'measurement__conversion', 'measurement__time', 'numbers__base_conversion', 'numbers__div_remainder', 'numbers__div_remainder_composed', 'numbers__gcd', 'numbers__gcd_composed', 'numbers__is_factor', 'numbers__is_factor_composed', 'numbers__is_prime', 'numbers__is_prime_composed', 'numbers__lcm', 'numbers__lcm_composed', 'numbers__list_prime_factors', 'numbers__list_prime_factors_composed', 'numbers__place_value', 'numbers__place_value_composed', 'numbers__round_number', 'numbers__round_number_composed', 'polynomials__add', 'polynomials__coefficient_named', 'polynomials__collect', 'polynomials__compose', 'polynomials__evaluate', 'polynomials__evaluate_composed', 'polynomials__expand', 'polynomials__simplify_power', 'probability__swr_p_level_set', 'probability__swr_p_sequence']\n",
    "len(math_dataset_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math_dataset_options = ['algebra__linear_1d', 'algebra__linear_1d_composed', 'algebra__linear_2d', 'algebra__linear_2d_composed', 'algebra__polynomial_roots', 'algebra__polynomial_roots_composed', 'algebra__sequence_next_term', 'algebra__sequence_nth_term', 'arithmetic__add_or_sub','arithmetic__add_or_sub_in_base', 'arithmetic__add_sub_multiple', 'arithmetic__div', 'arithmetic__mixed', 'arithmetic__mul', 'arithmetic__mul_div_multiple', 'arithmetic__nearest_integer_root', 'arithmetic__simplify_surd', 'calculus__differentiate', 'calculus__differentiate_composed', 'comparison__closest', 'comparison__closest_composed', 'comparison__kth_biggest', 'comparison__kth_biggest_composed', 'comparison__pair', 'comparison__pair_composed', 'comparison__sort', 'comparison__sort_composed', 'measurement__conversion', 'measurement__time', 'numbers__base_conversion', 'numbers__div_remainder', 'numbers__div_remainder_composed', 'numbers__gcd', 'numbers__gcd_composed', 'numbers__is_factor', 'numbers__is_factor_composed', 'numbers__is_prime', 'numbers__is_prime_composed', 'numbers__lcm', 'numbers__lcm_composed', 'numbers__list_prime_factors', 'numbers__list_prime_factors_composed', 'numbers__place_value', 'numbers__place_value_composed', 'numbers__round_number', 'numbers__round_number_composed', 'polynomials__add', 'polynomials__coefficient_named', 'polynomials__collect', 'polynomials__compose', 'polynomials__evaluate', 'polynomials__evaluate_composed', 'polynomials__expand', 'polynomials__simplify_power', 'probability__swr_p_level_set', 'probability__swr_p_sequence']\n",
    "math_dataset_options=['algebra__linear_1d','algebra__linear_2d']\n",
    "math_dataset={}\n",
    "\n",
    "for option in math_dataset_options:\n",
    "    math_dataset[option] = load_dataset(\"math_dataset\", option, split='train')\n",
    "    # if \"validation\" not in math_dataset[option].keys():\n",
    "    #     num_examples = math_dataset[option]['train'].num_rows\n",
    "    #     tmp = math_dataset[option]['train'].train_test_split(test_size=(int(num_examples*0.1))/num_examples)\n",
    "    #     math_dataset[option]['train'] = tmp['train']\n",
    "    #     math_dataset[option]['validation'] = tmp['test']\n",
    "    #     math_dataset[option].pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1799999\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 199999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dataset['algebra__linear_1d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"b'Solve -2054 - 13213 = -345*c + 18888 for c.\\\\n'\",\n",
       " 'answer': \"b'99\\\\n'\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dataset['algebra__linear_1d']['train'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MATH dataset\n",
    "\n",
    "https://huggingface.co/datasets/hendrycks/competition_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for competition_math contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/competition_math\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['problem', 'level', 'type', 'solution'],\n",
       "        num_rows: 7500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['problem', 'level', 'type', 'solution'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_math_dataset = load_dataset(\"competition_math\")\n",
    "competition_math_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=competition_math_dataset['test'].train_test_split(test_size=0.5)\n",
    "competition_math_dataset['validation']=tmp['test']\n",
    "competition_math_dataset['test']=tmp['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Evaluate $\\\\left\\\\lceil3\\\\left(6-\\\\frac12\\\\right)\\\\right\\\\rceil$.',\n",
       " 'level': 'Level 3',\n",
       " 'type': 'Algebra',\n",
       " 'solution': 'Firstly, $3\\\\left(6-\\\\frac12\\\\right)=18-1-\\\\frac12=17-\\\\frac12$.  Because $0\\\\le\\\\frac12<1$, we have $\\\\left\\\\lceil17-\\\\frac12\\\\right\\\\rceil=\\\\boxed{17}$.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_math_dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for competition_math_dataset, change key \"problem\" to \"question\" and \"solution\" to \"answer\"\n",
    "competition_math_dataset = competition_math_dataset.rename_column(\"problem\", \"question\")\n",
    "competition_math_dataset = competition_math_dataset.rename_column(\"solution\", \"answer\")\n",
    "# if \"validation\" not in competition_math_dataset.keys():\n",
    "#         num_examples = competition_math_dataset['train'].num_rows\n",
    "#         tmp = competition_math_dataset['train'].train_test_split(test_size=(int(num_examples*0.1))/num_examples)\n",
    "#         competition_math_dataset['train'] = tmp['train']\n",
    "#         competition_math_dataset['validation'] = tmp['test']\n",
    "#         competition_math_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'level', 'type', 'answer'],\n",
       "        num_rows: 7500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'level', 'type', 'answer'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'level', 'type', 'answer'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_math_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Evaluate $\\\\left\\\\lceil3\\\\left(6-\\\\frac12\\\\right)\\\\right\\\\rceil$.',\n",
       " 'level': 'Level 3',\n",
       " 'type': 'Algebra',\n",
       " 'answer': 'Firstly, $3\\\\left(6-\\\\frac12\\\\right)=18-1-\\\\frac12=17-\\\\frac12$.  Because $0\\\\le\\\\frac12<1$, we have $\\\\left\\\\lceil17-\\\\frac12\\\\right\\\\rceil=\\\\boxed{17}$.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_math_dataset['train'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stanford Question Answering Dataset (SQuAD)\n",
    "\n",
    "https://huggingface.co/datasets/rajpurkar/squad\n",
    "\n",
    "https://huggingface.co/datasets/rajpurkar/squad_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5285\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661181',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'What is the Grotto at Notre Dame?',\n",
       " 'answers': {'text': ['a Marian place of prayer and reflection'],\n",
       "  'answer_start': [381]}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5285/5285 [00:00<00:00, 6428.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "squad_dataset=squad_dataset.map(lambda x: {\"question\": x[\"question\"]+\"\\n\"+x[\"context\"], \"answer\": x[\"answers\"][\"text\"][0]})\n",
    "# num_examples = squad_dataset['train'].num_rows\n",
    "# tmp = squad_dataset['train'].train_test_split(test_size=(int(num_examples*0.1))/num_examples)\n",
    "# squad_dataset['train'] = tmp['train']\n",
    "# squad_dataset['validation'] = tmp['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56dfe78e7aa994140058e24a',\n",
       " 'title': 'Pub',\n",
       " 'context': \"Most British pubs still have decorated signs hanging over their doors, and these retain their original function of enabling the identification of the pub. Today's pub signs almost always bear the name of the pub, both in words and in pictorial representation. The more remote country pubs often have stand-alone signs directing potential customers to their door.\",\n",
       " 'question': \"What piece of information is almost always listed on a pub sign?\\nMost British pubs still have decorated signs hanging over their doors, and these retain their original function of enabling the identification of the pub. Today's pub signs almost always bear the name of the pub, both in words and in pictorial representation. The more remote country pubs often have stand-alone signs directing potential customers to their door.\",\n",
       " 'answers': {'text': ['the name of the pub'], 'answer_start': [192]},\n",
       " 'answer': 'the name of the pub'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2_dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=squad_v2_dataset['validation'].train_test_split(test_size=0.5)\n",
    "squad_v2_dataset['validation']=tmp['test']\n",
    "squad_v2_dataset['test']=tmp['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5a8d7bf7df8bba001a0f9ab1',\n",
       " 'title': 'The_Legend_of_Zelda:_Twilight_Princess',\n",
       " 'context': 'The Legend of Zelda: Twilight Princess (Japanese: ゼルダの伝説 トワイライトプリンセス, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b]',\n",
       " 'question': 'What category of game is Legend of Zelda: Australia Twilight?',\n",
       " 'answers': {'text': [], 'answer_start': []}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_v2_dataset['train'][2075]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/5937 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5937/5937 [00:00<00:00, 17452.71 examples/s]\n",
      "Map: 100%|██████████| 2950/2950 [00:00<00:00, 6843.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# clean all data whihc do not have answer(answer[text] is empty)\n",
    "squad_v2_dataset = squad_v2_dataset.filter(lambda x: x[\"answers\"][\"text\"] != [])\n",
    "squad_v2_dataset=squad_v2_dataset.map(lambda x: {\"question\": x[\"question\"]+\"\\n\"+x[\"context\"], \"answer\": x[\"answers\"][\"text\"][0]})\n",
    "# num_examples = squad_v2_dataset['train'].num_rows\n",
    "# tmp = squad_v2_dataset['train'].train_test_split(test_size=(int(num_examples*0.1))/num_examples)\n",
    "# squad_v2_dataset['train'] = tmp['train']\n",
    "# squad_v2_dataset['validation'] = tmp['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers', 'answer'],\n",
       "        num_rows: 86821\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers', 'answer'],\n",
       "        num_rows: 2950\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers', 'answer'],\n",
       "        num_rows: 2950\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_v2_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56bf6b0f3aeaaa14008c9601',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'In what city and state did Beyonce  grow up? \\nBeyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'answers': {'text': ['Houston, Texas'], 'answer_start': [166]},\n",
       " 'answer': 'Houston, Texas'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_v2_dataset['train'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Science Questions Answering (SciQ)\n",
    "\n",
    "https://huggingface.co/datasets/allenai/sciq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciq_dataset = load_dataset(\"sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the least dangerous radioactive decay?',\n",
       " 'distractor3': 'zeta decay',\n",
       " 'distractor1': 'beta decay',\n",
       " 'distractor2': 'gamma decay',\n",
       " 'correct_answer': 'alpha decay',\n",
       " 'support': 'All radioactive decay is dangerous to living things, but alpha decay is the least dangerous.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sciq_dataset, combine \"correct answer\" and \"support\" in a single key \"answer\"\n",
    "sciq_dataset = sciq_dataset.map(lambda x: {\"question\": x[\"question\"], \"answer\": x[\"correct_answer\"] + \". \" + x[\"support\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the least dangerous radioactive decay?',\n",
       " 'distractor3': 'zeta decay',\n",
       " 'distractor1': 'beta decay',\n",
       " 'distractor2': 'gamma decay',\n",
       " 'correct_answer': 'alpha decay',\n",
       " 'support': 'All radioactive decay is dangerous to living things, but alpha decay is the least dangerous.',\n",
       " 'answer': 'alpha decay. All radioactive decay is dangerous to living things, but alpha decay is the least dangerous.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset['train'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. AI2 Reasoning Challenge (ARC)\n",
    "\n",
    "https://huggingface.co/datasets/allenai/ai2_arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_subset=['ARC-Challenge','ARC-Easy']\n",
    "arc_dataset={}\n",
    "for subset in arc_subset:\n",
    "    arc_dataset[subset] = load_dataset(\"ai2_arc\", subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_7041615',\n",
       " 'question': 'Which of these do scientists offer as the most recent explanation as to why many plants and animals died out at the end of the Mesozoic era?',\n",
       " 'choices': {'text': ['worldwide disease',\n",
       "   'global mountain building',\n",
       "   'rise of mammals that preyed upon plants and animals',\n",
       "   'impact of an asteroid created dust that blocked the sunlight'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'D'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_dataset['ARC-Challenge']['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_item(item):\n",
    "    question = item['question']\n",
    "    choices = item['choices']['text']\n",
    "    labels = item['choices']['label']\n",
    "    formatted_question = f\"{question}\\n\"\n",
    "    for label, choice in zip(labels, choices):\n",
    "        formatted_question += f\"{label}. {choice}\\n\"\n",
    "    item['question'] = formatted_question.strip()  # Update the question field\n",
    "    item['answer'] = item['answerKey'] + \". \" + choices[labels.index(item['answerKey'])]\n",
    "    return item\n",
    "\n",
    "for subset in arc_subset:\n",
    "    arc_dataset[subset] = arc_dataset[subset].map(reformat_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'answer'],\n",
       "        num_rows: 1119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'answer'],\n",
       "        num_rows: 1172\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'answer'],\n",
       "        num_rows: 299\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_dataset['ARC-Challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'answer'],\n",
       "        num_rows: 2251\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'answer'],\n",
       "        num_rows: 2376\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'answer'],\n",
       "        num_rows: 570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_dataset['ARC-Easy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_7041615',\n",
       " 'question': 'Which of these do scientists offer as the most recent explanation as to why many plants and animals died out at the end of the Mesozoic era?\\nA. worldwide disease\\nB. global mountain building\\nC. rise of mammals that preyed upon plants and animals\\nD. impact of an asteroid created dust that blocked the sunlight',\n",
       " 'choices': {'text': ['worldwide disease',\n",
       "   'global mountain building',\n",
       "   'rise of mammals that preyed upon plants and animals',\n",
       "   'impact of an asteroid created dust that blocked the sunlight'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'D',\n",
       " 'answer': 'D. impact of an asteroid created dust that blocked the sunlight'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_dataset['ARC-Challenge']['train'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. wikipedia dataset (for RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. EPFL preference pairs dataset (for DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for MCQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to put our datasets in this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 'machine_learning', 'question': 'Question: MLE estimates are often undesirable because?\\n\\nOptions:\\nA. they are biased\\nB. they have high variance\\nC. they are not consistent estimators\\nD. None of the above\\n\\nAnswer:', 'answer': 'B'}\n",
      "Question: MLE estimates are often undesirable because?\n",
      "\n",
      "Options:\n",
      "A. they are biased\n",
      "B. they have high variance\n",
      "C. they are not consistent estimators\n",
      "D. None of the above\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "example_instance = {\"subject\": \"machine_learning\", \"question\": \"Question: MLE estimates are often undesirable because?\\n\\nOptions:\\nA. they are biased\\nB. they have high variance\\nC. they are not consistent estimators\\nD. None of the above\\n\\nAnswer:\", \"answer\": \"B\"}\n",
    "print(example_instance)\n",
    "print(example_instance['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = examples['question']\n",
    "    answers = examples['answer']\n",
    "    inputs = tokenizer(questions, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(answers, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = math_dataset.map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate into single MCQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 9239.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 7500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 19179\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 3500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 3500\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 22549\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 7048\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 4369\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2950/2950 [00:00<00:00, 14230.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 109370\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 9998\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 7319\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "competition_math_dataset_chatformate = competition_math_dataset.map(create_conversation, remove_columns=competition_math_dataset['train'].features,batched=False)\n",
    "final_dataset = competition_math_dataset_chatformate.shuffle()\n",
    "print(final_dataset)\n",
    "\n",
    "datasets_list = []\n",
    "sciq_dataset_chatformate = sciq_dataset.map(create_conversation, remove_columns=sciq_dataset['train'].features,batched=False)\n",
    "datasets_list.append(sciq_dataset_chatformate)\n",
    "\n",
    "for key in [\"train\",\"validation\",\"test\"]:\n",
    "    final_dataset[key] = concatenate_datasets([final_dataset[key], datasets_list[0][key]]).shuffle()\n",
    "\n",
    "print(final_dataset)\n",
    "\n",
    "datasets_list = []\n",
    "datasets_list.append(final_dataset)\n",
    "\n",
    "arc_dataset_chatformate={}\n",
    "for subset in arc_subset:\n",
    "    arc_dataset_chatformate[subset] = arc_dataset[subset].map(create_conversation, remove_columns=arc_dataset[subset]['train'].features,batched=False)\n",
    "datasets_list.extend(list(arc_dataset_chatformate.values()))\n",
    "for key in [\"train\",\"validation\",\"test\"]:\n",
    "    final_dataset[key] = concatenate_datasets([dataset[key] for dataset in datasets_list]).shuffle()\n",
    "print(final_dataset)\n",
    "\n",
    "datasets_list = []\n",
    "squad_v2_dataset_chatformate = squad_v2_dataset.map(create_conversation, remove_columns=squad_v2_dataset['train'].features,batched=False)\n",
    "datasets_list.append(squad_v2_dataset_chatformate)\n",
    "\n",
    "for key in [\"train\",\"validation\",\"test\"]:\n",
    "    final_dataset[key] = concatenate_datasets([final_dataset[key], datasets_list[0][key]]).shuffle()\n",
    "print(final_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 109370\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 9998\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 7319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format:   0%|          | 0/110 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 110/110 [00:02<00:00, 52.80ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 61.60ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 53.81ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6748124"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset['train'].to_json('sft_train_dataset.json', orient=\"records\")\n",
    "final_dataset['validation'].to_json('sft_validation_dataset.json', orient=\"records\")\n",
    "final_dataset['test'].to_json('sft_test_dataset.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for option in math_dataset_options:\n",
    "    i+=1\n",
    "    print(\"sub datset:\",str(i)+\" \"+option)\n",
    "    train_file_path = f\"sft_train_dataset_{option}.json\"\n",
    "    validation_file_path = f\"sft_validation_dataset_{option}.json\"\n",
    "\n",
    "    math_dataset_chatformate={}\n",
    "    #take only 10% of the dataset\n",
    "    math_dataset[option] = math_dataset[option].train_test_split(test_size=0.1)\n",
    "    \n",
    "\n",
    "    math_dataset_chatformate[option] = math_dataset[option].map(create_conversation, remove_columns=math_dataset[option]['train'].features,batched=False)\n",
    "    math_dataset_chatformate[option][\"train\"].to_json(train_file_path, orient=\"records\")\n",
    "    math_dataset_chatformate[option][\"validation\"].to_json(validation_file_path, orient=\"records\")\n",
    "\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 1 algebra__linear_1d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  13%|█▎        | 228689/1799999 [00:21<02:26, 10702.49 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m validation_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msft_validation_dataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m math_dataset_chatformate\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m----> 9\u001b[0m math_dataset_chatformate[option] \u001b[38;5;241m=\u001b[39m \u001b[43mmath_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43moption\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_conversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43moption\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m math_dataset_chatformate[option][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_json(train_file_path, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m math_dataset_chatformate[option][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_json(validation_file_path, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/dataset_dict.py:868\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 868\u001b[0m     {\n\u001b[1;32m    869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    868\u001b[0m     {\n\u001b[0;32m--> 869\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3457\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3456\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3457\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m   3458\u001b[0m         example \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m   3459\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2399\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[0;32m-> 2399\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2794\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2792\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2796\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2797\u001b[0m )\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/formatting/formatting.py:588\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    586\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m _query_table(table, key)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43m_query_table_with_indices_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa_subtable\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp_310/lib/python3.10/site-packages/datasets/formatting/formatting.py:60\u001b[0m, in \u001b[0;36m_query_table_with_indices_mapping\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mQuery a pyarrow Table to extract the subtable that correspond to the given key.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mThe :obj:`indices` parameter corresponds to the indices mapping in case we cant to take into\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03maccount a shuffling or an indices selection for example.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mThe indices table must contain one column named \"indices\" of type uint64.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _query_table(table, key)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for option in math_dataset_options:\n",
    "    i+=1\n",
    "    print(\"sub datset:\",str(i)+\" \"+option)\n",
    "    train_file_path = f\"sft_train_dataset_{option}.json\"\n",
    "    validation_file_path = f\"sft_validation_dataset_{option}.json\"\n",
    "\n",
    "    math_dataset_chatformate={}\n",
    "    math_dataset_chatformate[option] = math_dataset[option].map(create_conversation, remove_columns=math_dataset[option]['train'].features,batched=False)\n",
    "    math_dataset_chatformate[option][\"train\"].to_json(train_file_path, orient=\"records\")\n",
    "    math_dataset_chatformate[option][\"validation\"].to_json(validation_file_path, orient=\"records\")\n",
    "\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 109370 examples [00:00, 392844.35 examples/s]\n",
      "Generating validation split: 7319 examples [00:00, 399221.16 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 109370\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 7319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset=load_dataset(\"json\", data_files={\"train\": \"SFT_data/sft_train_dataset.json\", \"validation\": \"SFT_data/sft_validation_dataset.json\"})\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 1 algebra__linear_1d/56\n",
      "sub datset: 2 algebra__linear_1d_composed/56\n",
      "sub datset: 3 algebra__linear_2d/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1799999 examples [00:01, 1564835.52 examples/s]\n",
      "Generating validation split: 199999 examples [00:00, 1517221.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 4 algebra__linear_2d_composed/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1799999 examples [00:01, 1175372.11 examples/s]\n",
      "Generating validation split: 199999 examples [00:00, 1189261.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 5 algebra__polynomial_roots/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1799999 examples [00:01, 1390340.65 examples/s]\n",
      "Generating validation split: 199999 examples [00:00, 1330019.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 6 algebra__polynomial_roots_composed/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1799999 examples [00:01, 1289591.24 examples/s]\n",
      "Generating validation split: 199999 examples [00:00, 1110450.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 7 algebra__sequence_next_term/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1799999 examples [00:01, 1567706.03 examples/s]\n",
      "Generating validation split: 199999 examples [00:00, 1550217.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 8 algebra__sequence_nth_term/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1799999 examples [00:01, 1252343.33 examples/s]\n",
      "Generating validation split: 199999 examples [00:00, 1021385.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 9 arithmetic__add_or_sub/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1799999 examples [00:01, 1734000.94 examples/s]\n",
      "Generating validation split: 199999 examples [00:00, 1595260.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub datset: 10 arithmetic__add_or_sub_in_base/56\n",
      "sub datset: 11 arithmetic__add_sub_multiple/56\n",
      "sub datset: 12 arithmetic__div/56\n",
      "sub datset: 13 arithmetic__mixed/56\n",
      "sub datset: 14 arithmetic__mul/56\n",
      "sub datset: 15 arithmetic__mul_div_multiple/56\n",
      "sub datset: 16 arithmetic__nearest_integer_root/56\n",
      "sub datset: 17 arithmetic__simplify_surd/56\n",
      "sub datset: 18 calculus__differentiate/56\n",
      "sub datset: 19 calculus__differentiate_composed/56\n",
      "sub datset: 20 comparison__closest/56\n",
      "sub datset: 21 comparison__closest_composed/56\n",
      "sub datset: 22 comparison__kth_biggest/56\n",
      "sub datset: 23 comparison__kth_biggest_composed/56\n",
      "sub datset: 24 comparison__pair/56\n",
      "sub datset: 25 comparison__pair_composed/56\n",
      "sub datset: 26 comparison__sort/56\n",
      "sub datset: 27 comparison__sort_composed/56\n",
      "sub datset: 28 measurement__conversion/56\n",
      "sub datset: 29 measurement__time/56\n",
      "sub datset: 30 numbers__base_conversion/56\n",
      "sub datset: 31 numbers__div_remainder/56\n",
      "sub datset: 32 numbers__div_remainder_composed/56\n",
      "sub datset: 33 numbers__gcd/56\n",
      "sub datset: 34 numbers__gcd_composed/56\n",
      "sub datset: 35 numbers__is_factor/56\n",
      "sub datset: 36 numbers__is_factor_composed/56\n",
      "sub datset: 37 numbers__is_prime/56\n",
      "sub datset: 38 numbers__is_prime_composed/56\n",
      "sub datset: 39 numbers__lcm/56\n",
      "sub datset: 40 numbers__lcm_composed/56\n",
      "sub datset: 41 numbers__list_prime_factors/56\n",
      "sub datset: 42 numbers__list_prime_factors_composed/56\n",
      "sub datset: 43 numbers__place_value/56\n",
      "sub datset: 44 numbers__place_value_composed/56\n",
      "sub datset: 45 numbers__round_number/56\n",
      "sub datset: 46 numbers__round_number_composed/56\n",
      "sub datset: 47 polynomials__add/56\n",
      "sub datset: 48 polynomials__coefficient_named/56\n",
      "sub datset: 49 polynomials__collect/56\n",
      "sub datset: 50 polynomials__compose/56\n",
      "sub datset: 51 polynomials__evaluate/56\n",
      "sub datset: 52 polynomials__evaluate_composed/56\n",
      "sub datset: 53 polynomials__expand/56\n",
      "sub datset: 54 polynomials__simplify_power/56\n",
      "sub datset: 55 probability__swr_p_level_set/56\n",
      "sub datset: 56 probability__swr_p_sequence/56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 101058/101058 [33:09<00:00, 50.80ba/s] \n",
      "Creating json from Arrow format: 100%|██████████| 11229/11229 [03:14<00:00, 57.62ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1852263434"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset=load_dataset(\"json\", data_files={\"train\": \"sft_train_dataset.json\", \"validation\": \"sft_validation_dataset.json\"})\n",
    "i=0\n",
    "for option in math_dataset_options:\n",
    "    i+=1\n",
    "    print(\"sub datset:\",str(i)+\" \"+option+\"/\"+str(len(math_dataset_options)))\n",
    "    train_file_path = f\"sft_train_dataset_{option}.json\"\n",
    "    validation_file_path = f\"sft_validation_dataset_{option}.json\"\n",
    "    math_dataset_chatformate_sub = load_dataset(\"json\", data_files={\"train\": train_file_path, \"validation\": validation_file_path})\n",
    "    for key in [\"train\",\"validation\"]:\n",
    "        final_dataset[key] = concatenate_datasets([final_dataset[key], math_dataset_chatformate_sub[key]]).shuffle()\n",
    "\n",
    "final_dataset[\"train\"].to_json(\"sft_train_dataset_all.json\", orient=\"records\")\n",
    "final_dataset[\"validation\"].to_json(\"sft_validation_dataset_all.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
