{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a22c3dd13d4aada410f18b59d7b5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code origin\n",
    "#\n",
    "#Author: Alexander Valentini  (alexander.valentini@epfl.ch)\n",
    "#Made on: 26/5-2024\n",
    "#Made for: Project report for course CS-552. submission on 2/6-2024\n",
    "#Jessica started this file at an early stage for training, but all subsequent data analysis and filtering was done by Alexander\n",
    "#This file was used for debugging done by Alexander.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorWithPadding#, BitsAndBytesConfig \n",
    "from datasets import load_from_disk, load_dataset, Dataset\n",
    "import numpy as np\n",
    "#Using quantization, but this should probably not be in the final version:\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from trl import DPOTrainer\n",
    "\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    llm_int8_threshold=6.0,\n",
    "#    llm_int8_has_fp16_weight=False,\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#)\n",
    "\n",
    "model_name = 'stabilityai/stablelm-zephyr-3b'\n",
    "safetensors_path = 'sft_stablelm_zephyr_3b/25.05.2024_new_data_and_model/checkpoint-20000'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "model = AutoModelForCausalLM.from_pretrained(safetensors_path, attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16, use_safetensors = True, device_map=\"auto\")\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"sdpa\",\n",
    "#    torch_dtype=torch.bfloat16, use_safetensors = True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableLmForCausalLM(\n",
       "  (model): StableLmModel(\n",
       "    (embed_tokens): Embedding(50304, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x StableLmDecoderLayer(\n",
       "        (self_attn): StableLmSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (rotary_emb): StableLmRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): StableLmMLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "List 3 synonyms for the word \"tiny\"<|endoftext|>\n",
      "<|assistant|>\n",
      "B. The Sun of the first of carbon has the heart of a nucleus. The earliest occurs are\n"
     ]
    }
   ],
   "source": [
    "prompt = [{'role': 'user', 'content': 'List 3 synonyms for the word \"tiny\"'}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokens = model.generate(\n",
    "    inputs.to(model.device),\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in training and evaluation data:\n",
    "import os\n",
    "\n",
    "project_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_dir = os.path.join(project_dir, 'data/dpo_preference_data/processed/EPFL_preference_data')\n",
    "preference_data = load_from_disk(data_dir)\n",
    "train_dataset = preference_data['train']\n",
    "eval_dataset = preference_data['eval']\n",
    "#train_dataset\n",
    "#print(f\"Number of examples in the training set: {len(train_dataset)}\")\n",
    "#print(f\"Number of examples in the evaluation set: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|user|>\\nQuestion: Let $X$ denote the random variable associated to the plaintexts and $Y$ the random variable associated to the corresponding ciphertexts. If a cryptosystem achieves perfect secrecy, then we have that \\\\dots?\\n\\nOptions:\\nA. $\\\\Pr [X=x|Y=y] = \\\\Pr[Y=y]$.\\nB. $\\\\Pr [X=x|Y=y] = \\\\Pr[X=x]$.\\nC. $\\\\Pr [X=x|Y=y] = \\\\Pr[X=x,Y=y]$.\\nD. $\\\\Pr [X=x] = \\\\Pr[Y=y]$.<|endoftext|>\\n<|assistant|>\\n',\n",
       " 'chosen': 'The correct statement is: \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x]$.\\\\\"\\\\n\\\\nPerfect secrecy in a cryptosystem means that observing the ciphertext does not give any information about the plaintext. Mathematically, this is formalized as $\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x]$, meaning that the probability of a certain plaintext being the true message given the ciphertext is the same as the probability of that plaintext being the true message without knowing the ciphertext.\\\\n\\\\nThe other statements are not necessarily true in the context of perfect secrecy. \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[Y=y]$\\\\\" is not correct because if the probability of the plaintext given the ciphertext was equal to the probability of the ciphertext, it would mean that the ciphertext reveals information about the plaintext. \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x,Y=y]$\\\\\" is also not correct because it implies that the probability of the plaintext given the ciphertext is the same as the joint probability of the plaintext and ciphertext, which may not be true. Finally, \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x] = \\\\\\\\\\\\\\\\Pr[Y=y]$\\\\\" is not necessarily true as well because perfect secrecy does not require the marginal probabilities of the plaintexts and ciphertexts to be equal.<|endoftext|>\\n',\n",
       " 'rejected': 'I apologize for the mistake in my previous response. The correct statement should be: \\n\\nCorrect statement: \"$\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\Pr[X=x]$.\"\\n\\nJustification: In a cryptosystem that achieves perfect secrecy, the knowledge of the ciphertext does not reveal any information about the plaintext. Therefore, the probability of a specific plaintext given a specific ciphertext should be the same as the probability of that specific plaintext occurring. This is reflected in the correct statement. Thank you for pointing out the error.<|endoftext|>\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e1e8131deb4eae9c133d1699f68f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e479f3f6ef484e62adafe36de284173a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/20 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61523bc0f2994242ac9be4ca5301ef57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/26801833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3f6b6f1f4a49fc868825a44303b6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e852a46ac4e34b1586487a517f35eb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_stack_exchange_paired(\n",
    "    data_dir: str = \"data/rl\",\n",
    "    sanity_check: bool = False,\n",
    "    cache_dir: Optional[str] = None,\n",
    "    num_proc=24,\n",
    ") -> Dataset:\n",
    "    \"\"\"Load the stack-exchange-paired dataset from Hugging Face and convert it to the necessary format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts are structured as follows:\n",
    "      \"Question: \" + <prompt> + \"\\n\\nAnswer: \"\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"lvwerra/stack-exchange-paired\",\n",
    "        split=\"train\",\n",
    "        cache_dir=cache_dir,\n",
    "        data_dir=data_dir,\n",
    "        verification_mode=\"no_checks\",\n",
    "    )\n",
    "    original_columns = dataset.column_names\n",
    "\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def return_prompt_and_responses(samples) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"question\"]],\n",
    "            \"chosen\": samples[\"response_j\"],\n",
    "            \"rejected\": samples[\"response_k\"],\n",
    "        }\n",
    "\n",
    "    return dataset.map(\n",
    "        return_prompt_and_responses,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=original_columns,\n",
    "    )\n",
    "\n",
    "train_dataset = get_stack_exchange_paired(data_dir=\"data/rl\", sanity_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Question: I\\'m writing a code using Java Swing to press the right button when I type a number key.\\nBut I can\\'t find what I want through search.\\nThis is my code and I can\\'t understand why this isn\\'t working.\\nPlease help me..\\n\\n```\\nimport javax.swing.*;\\nimport java.awt.Dimension;\\nimport java.awt.event.*;\\n\\nclass class01 {\\n\\n    public static void main(String[] args) {\\n\\n        JFrame f = new JFrame(\"Key event test\"); \\n        f.setSize(230, 500);\\n        f.setLayout(null);\\n        f.setVisible(true);\\n        f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\\n\\n        JLabel label = new JLabel(); \\n\\n        JButton button1 = new JButton(\"Coffe\"); \\n        button1.setSize(100, 100);\\n        button1.setLocation(0, 0);\\n\\n        JButton button2 = new JButton(\"Latte\");\\n        button2.setSize(100, 100);\\n        button2.setLocation(0, 100);\\n\\n        JButton button3 = new JButton(\"Espresso\");\\n        button3.setSize(100, 100);\\n        button3.setLocation(100, 100);\\n\\n        JButton button4 = new JButton(\"Vanilla Latte\");\\n        button4.setSize(100, 100);\\n        button4.setLocation(100, 0);\\n\\n        f.add(button1); \\n        f.add(button2);\\n        f.add(button3);\\n        f.add(button4);\\n\\n        // Show message when the corresponding button is pressed.\\n        button1.addActionListener(new ActionListener() {\\n            public void actionPerformed(ActionEvent arg0) {\\n                button1.keyPressed(KeyEvent.VK_1);\\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\\n            }\\n        });\\n        button2.addActionListener(new ActionListener() {\\n            public void actionPerformed(ActionEvent arg0) {\\n                button2.keyPressed(KeyEvent.VK_2);\\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Latte selected\");\\n            }\\n        });\\n        button3.addActionListener(new ActionListener() {\\n            public void actionPerformed(ActionEvent arg0) {\\n                button3.keyPressed(KeyEvent.VK_3);\\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Espresso selected\");\\n            }\\n        });\\n        button4.addActionListener(new ActionListener() {\\n            public void actionPerformed(ActionEvent arg0) {\\n                button4.keyPressed(KeyEvent.VK_4);\\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Vanilla Latte selected\");\\n            }\\n        });\\n    }\\n}\\n\\n```\\n\\nAnswer: ',\n",
       " 'chosen': 'The code you are showing does exactly one thing: attach action listeners to your buttons..\\n\\nMeaning: when you click the button, then the listener will be called. \\n\\nYou need a generic keyboard listener that translates key events into calls to the appropriate button, respectively action listener instead.',\n",
       " 'rejected': 'When you do this:\\n\\n```\\nbutton1.addActionListener(new ActionListener() {\\n        public void actionPerformed(ActionEvent arg0) {\\n            button1.keyPressed(KeyEvent.VK_1);\\n            JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\\n        }\\n    });\\n\\n```\\n\\nYou are telling `button1` what to do when somebody clicks on the button. The line with `keyPressed` should not be there (it does not compile even).\\n\\nWhat you need to do is listen for key presses by adding a `KeyListener` to the frame like this:\\n\\n```\\nf.addKeyListener(new KeyAdapter() {\\n        @Override\\n        public void keyTyped(KeyEvent e) {\\n            if( e.getKeyChar() == KeyEvent.VK_1) {\\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\\n            }\\n        }\\n    });\\n\\n```\\n\\nI repeated the `showMessageDialog`, but you should extract the actual logic into a method and call that method from within the `KeyListener` on the frame and the `ActionListener` on the button.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Lora:\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Training arguments for DPO:\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/dpo_model_test\",               # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=12,         # batch size per device during training\n",
    "    per_device_eval_batch_size=4,           # batch size for evaluation\n",
    "    gradient_accumulation_steps=1,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    learning_rate=5e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=25,                       # log every 25 steps\n",
    "    save_steps=400,                         # when to save checkpoint\n",
    "    save_total_limit=4,                     # limit the total amount of checkpoints\n",
    "    evaluation_strategy=\"steps\",            # evaluate every 1000 steps\n",
    "    eval_steps=700,                         # when to evaluate\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    " \n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "    \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2082 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773\n",
      "729\n",
      "803\n"
     ]
    }
   ],
   "source": [
    "#Finding 99th percentile of data length for padding:\n",
    "prompt_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 99))\n",
    "rejected_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"rejected\"]], 99))\n",
    "chosen_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"chosen\"]], 99))\n",
    "\n",
    "print(prompt_length)\n",
    "print(rejected_length)\n",
    "print(chosen_length)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This should be increased in later versions:\n",
    "prompt_max_length = 900\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\trl\\trainer\\dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffafc603fccb4c60a3fc70b46dfb178d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0707d70e7f1c42b09705f186b4e0e95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None, # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_max_length,\n",
    "    beta=dpo_args[\"beta\"],\n",
    "    loss_type=dpo_args[\"loss_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bf06defd994d70848165e5867c0493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\transformers\\models\\stablelm\\modeling_stablelm.py:485: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = Path(args.output_dir)\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda path: int(path.name.split(\"-\")[-1]))\n",
    "        print(f\"Loading from checkpoint {latest_checkpoint}\")\n",
    "    else:\n",
    "        latest_checkpoint = None\n",
    "else:\n",
    "    latest_checkpoint = None\n",
    "\n",
    "# Continue training from the latest checkpoint if available\n",
    "if latest_checkpoint:\n",
    "    trainer.train(resume_from_checkpoint=str(latest_checkpoint))\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "# save model at the end of training\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "tokenizer.save_pretrained(args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
